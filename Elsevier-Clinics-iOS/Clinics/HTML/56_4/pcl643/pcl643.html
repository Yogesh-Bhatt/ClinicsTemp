<html xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:cellrefs="java:eslo.extxslt.cell.content.Article" xmlns:dochead="java:eslo.extxslt.cell.content.Dochead" xmlns:eln="urn:com.elsevier.elslon.ja50.enhanced-links" xmlns:ent="urn:com.elsevier.elslon.ja50.entities" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:months="http://elsevier.co.uk/namespaces/cell/months" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:urlencoder="java:java.net.URLEncoder" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:str="http://xsltsl.org/string" xmlns:lcr="http://schema.elsevier.com/elslon/lcr/1.5/journal/output">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>EWT</title>
<link rel="stylesheet" href="style.css" type="text/css" />
</head>
<body style="padding:10px; color: #033b73;">
<hr />
<p id="lv_0001" class="level11">
<small>
<small>ARTICLE INFO</small>
</small>
</p>
<hr />
<p class="info1">
<i>Article history:</i>
<br />
©
2009 Elsevier Inc.. All rights reserved.<br />S0031-3955(09)00066-2
- see front matter ©
2009 Elsevier Inc.. All rights reserved.10.1016/j.pcl.2009.05.012 </p>
<hr />
<p class="tit">Model for Improvement - Part Two: Measurement and Feedback for Quality Improvement Efforts</p>
<p class="au">Greg Randolph MD, MPH <sup>a</sup> <sup>b</sup> <sup>c</sup> <sup>*</sup>, Megan Esporas MPH <sup>a</sup> <sup>b</sup> <sup>c</sup>, Lloyd Provost MS <sup>d</sup>, Sara Massie MPH <sup>a</sup> <sup>e</sup>, David G. Bundy MD, MPH <sup>f</sup></p>
<p class="aff" id="aff1">
<sup>a</sup> Department of Pediatrics, North Carolina Children's Center for Clinical Excellence, North Carolina Children's Hospital, CB# 7230, Chapel Hill, NC 27599-7230, USA</p>
<p class="aff" id="aff2">
<sup>b</sup> Department of Pediatrics, University of North Carolina at Chapel Hill, CB# 7230, Chapel Hill, NC 27599-7230, USA</p>
<p class="aff" id="aff3">
<sup>c</sup> Public Health Leadership Program, University of North Carolina at Chapel Hill, CB# 7230, Chapel Hill, NC 27599-7230, USA</p>
<p class="aff" id="aff4">
<sup>d</sup> API –Austin, 115 East Fifth Street, Suite 300, Austin, TX 78701, USA</p>
<p class="aff" id="aff5">
<sup>e</sup> Child Health Research Program, N.C. Translational and Clinical Sciences Institute, University of North Carolina at Chapel Hill, CB# 7230, Chapel Hill, NC 27599-7230, USA</p>
<p class="aff" id="aff6">
<sup>f</sup> Department of Pediatrics, Johns Hopkins University School of Medicine, CMSC 2-121, 600 N Wolfe Street, Baltimore, MD, USA</p>
<p class="aff" id="cor1">
<sup>*</sup> Corresponding author. Department of Pediatrics, University of North Carolina at Chapel Hill, CB# 7230, Chapel Hill, NC 27599 7230. <i>E-mail address:</i> <a href="mailto:randolph@unc.edu">randolph@unc.edu</a></p>
<p id="lv_0002" class="level1">
<small>Keywords</small>
</p>
<p class="normal">Quality improvement, Measurement, Feedback, Health care quality, Organizational performance</p>
<div class="section-separator" />
<div class="ja50-ce-abstract">
<div class="ja50-ce-abstract-section">
<p class="normal">Measurement and feedback are fundamental to quality improvement. There is a knowledge gap among health care professionals in knowing how to measure the impact of their quality improvement projects and how to use these data to improve care. This article presents a pragmatic approach to measurement and feedback for quality improvement efforts in local health care settings, such as hospitals or clinical practices. The authors include evidence-based strategies from health care and other industries, augmented with practical examples from the authors' collective years of experience designing measurement and feedback strategies.</p>
</div>
</div>
<p id="lv_0003" class="level1">
<div id="sec1" />
<p id="lv_0004" class="level2">Why this topic and why now?</p>
<p class="indent">The public, government, payors, and health care professionals increasingly agree that the quality of health care in the United States is in urgent need of improvement.<a title="" href="#bib1" name="back-bib1"><sup>1</sup></a> Measurement and feedback are fundamental aspects of quality improvement (QI); thus, national and local health care organizations are paying more attention to the selection and use of quality measures. To date, most of the attention and effort has been directed at developing measures at the national level to compare the performance of physicians and health care organizations, ie, what is referred to as national performance measures or standards. Little attention has been directed at measurement and feedback to guide QI projects, which represents the frontline of QI work. A substantial knowledge gap exists among health care professionals about how to select and use measures to guide QI projects.</p>
<p class="indent">In this article, the authors attempt to address this gap by providing evidence-based strategies from health care and other industries, augmented with practical examples from the authors' collective years of experience designing measurement and feedback strategies for frontline health care improvement teams. The article's focus is on the design and development of measurement and feedback strategies for QI projects in local settings, such as health care systems, hospitals, and clinical practices. The article also briefly discusses organizational measures, such as those included in dashboards or scorecards, which guide decision-making and priority setting for system-wide improvements.</p>
<p class="indent">The authors use a broad definition of quality measures, including not only clinical, but also satisfaction and financial measures, which are critical to health care organizations.<a title="" href="#bib2" name="back-bib2"><sup>2</sup></a> This focus aligns with the Institute of Medicine's definition of quality: “The degree to which health services for individuals and populations increase the likelihood of desired health outcomes and are consistent with current professional knowledge.”<a title="" href="#bib1" name="back-bib1"><sup>1</sup></a></p>
<p class="indent">This article does not address measurement for QI research, which is typically too slow, too expensive, too much of a data collection burden, and too complex for clinicians and staff engaged in QI efforts in local settings.<a title="" href="#bib3" name="back-bib3"><sup>3</sup></a> In addition, the authors do not address national performance standards used for quality assurance and accreditation, such as those established by the Joint Commission and the Center for Medicare &amp; Medicaid Services (CMS). These types of data are regularly collected and reported by hospitals, but they are infrequently used, and rarely sufficient, to guide improvement projects. For example, accountability measures are insufficient for QI teams because they are designed for external groups (eg, payors and regulators for comparison or public reassurance) and typically focus on only outcomes, not processes, making it impossible to link any process changes to changes in outcomes.<a title="" href="#bib3" name="back-bib3"><sup>3</sup></a></p>
<div id="sec2" />
<p id="lv_0005" class="level2">Background and history</p>
<p class="indent">The current use of measurement and feedback in health care QI originates from both medicine and industrial/business QI. In the mid-1800s, two pioneers, Florence Nightingale and John Snow, introduced the use of data in health care. Nightingale used measures of mortality to document the low quality of care injured British soldiers received during the Crimean War in Turkey. These data provided a compelling argument for reform by demonstrating that better sanitation in the field hospital could prevent deaths. Subsequent field hospital reforms reduced the death rate from greater than 40% to less than 5%. Nightingale also developed graphical methods to present data, clearly demonstrating that measures provide an organized way of approaching improvements in medical and surgical practice. Nightingale's achievements led to the adoption of formal record keeping in British hospitals.</p>
<p class="indent">Also in the mid-1800s, John Snow tracked the incidence and geographic location of cholera in London. By mapping the clusters of cholera cases, Snow provided convincing evidence that homes supplied by the Broad Street water pump had a much higher incidence of cholera than homes supplied with water from other sources. Despite the lack of a clear causal explanation (Snow postulated a “cholera poison”), the data demonstrating the pattern of disease outbreak was convincing enough to persuade authorities to remove the handle from the Broad Street pump, which greatly reduced the incidence of cholera.</p>
<p class="indent">The first American to publish on performance measurement in health care was Ernest Codman, a surgeon at Massachusetts General Hospital. Codman advocated for hospital reform and systematic health care performance assessment, and in 1910, proposed the “end result system of hospital standardization,”<a title="" href="#bib4" name="back-bib4"><sup>4</sup></a> whereby a hospital would track every patient it treated long enough to determine whether treatment was effective. By tracking patient outcomes, adverse events could be identified and changes could be made to improve the care of future patients.</p>
<p class="indent">Modern QI science originated in the 1920s and is often attributed to the work of Walter Shewhart of Bell Telephone Laboratories.<a title="" href="#bib5" name="back-bib5"><sup>5</sup></a> Shewhart's work made measurement fundamental to QI activities. He published principles and techniques basic to the use of measurement, including concepts of statistical control, operational definitions, and visual display of data. He postulated two rules for presentation of data:<span class="ja50-ce-list"><span class="list-item"><span class="ja50-ce-list-item-label">1)</span> <span class="list-item-content">Data has no meaning apart from its context.</span></span><span class="list-item"><span class="ja50-ce-list-item-label">2)</span> <span class="list-item-content">Data contains both signal and noise; to be able to learn, one must separate the signal from the noise.</span></span></span></p>
<p class="indent">One of Shewhart's colleagues, W. Edwards Deming, an American statistician and physicist, popularized his ideas in the industrial production and management sector beginning in the 1940s.<a title="" href="#bib6" name="back-bib6"><sup>6</sup></a> Deming applied Shewhart's concepts in government and industry, described the Plan-Do-Study-Act (PDSA) Cycle as an approach to QI, and taught these concepts and methods to thousands of people, primarily in industries other than health care.</p>
<p class="indent">In the 1970s, physician Avedis Donabedian proposed a model for assessing health care quality, describing seven pillars of quality: efficacy, efficiency, optimality, acceptability, legitimacy, equity, and cost. He posited, “<i>Structure</i> is the environment in which health care is provided, <i>process</i> is the method by which health care is provided, and <i>outcome</i> as the result of the care provided.”<a title="" href="#bib7" name="back-bib7"><sup>7</sup></a> Focusing on structure, process, and outcome, he emphasized the importance of measurement and evaluation of health care quality, assuring completeness and accuracy of medical records, observer bias, patient satisfaction, and cultural preferences for health care.</p>
<p class="indent">The 1980s saw an emphasis on outcome measurement in the health care literature as a result of variability in medical practice, evidence-based medicine, and regulatory agency requirements. During this time, physicians Paul Batalden and Don Berwick were among many health professionals who began to study and apply Deming's ideas to health care. In 1991, Batalden and Berwick helped form the Institute for Healthcare Improvement (IHI), which has led the application of QI science to health care in the United States and internationally.<a title="" href="#bib8" name="back-bib8"><sup>8</sup></a> The IHI has been influential in promoting the adoption of measurement and feedback to improve the quality of health care.</p>
<div id="sec3" />
<p id="lv_0006" class="level2">The fundamentals of quality improvement projects</p>
<p class="indent">A cornerstone of QI science is the concept of a system.<a name="back-bib9" href="#bib9"><sup>9–12</sup></a> Systems are complex and dynamic, and a system's nature has a profound effect on the measurement and feedback of QI projects undertaken within it. A system is “an interdependent group of items, people, or processes working together toward a common purpose”. A system's identification of its common purpose aligns its parts. Those parts are interdependent, meaning that all parts of the system, and all relations between and among those parts, can influence system performance. Thus, QI projects require multiple measures, at multiple levels, to understand the effects of change on the different components of the system, and on the system as a whole.</p>
<p class="indent">The aim of a QI project must guide the selection of project measures. Project measures must be specific to the processes and outcomes being targeted for improvement and specific to the local target audience (within a health care system, hospital, or clinical practice). Stakeholders, those who can affect or be affected by changes in a system, especially frontline clinicians and staff, should receive feedback on the QI effort to understand how the work is helping to achieve the improvement aim. This type of measurement and feedback answers the fundamental question, “Are the changes we are making (via QI projects) leading to improvement?”</p>
<p class="indent">In the following sections, the authors describe the process of developing, testing, and implementing measurement and feedback strategies in a typical QI project. <a title="" href="#fig1" name="back-fig1" class="ja50-ce-cross-ref">Fig. 1</a> presents a summary of this process, and a glossary of QI terms used in our discussion is available at the end of the article.</p>
<div style="border: 1px solid #000000; padding: 5px; margin: 10px 0px ">
<a href="gr1_lrg.jpg">
<p class="img">
<img src="gr1.jpg" alt="image" id="fig1" />
</p>
</a>
<p class="caption">
<b>Fig. 1</b>   <span class="caption">Flow chart demonstrating the stages of development, testing, and implementation of a measurement and feedback plan during a typical QI project.<br /></span></p>
</div>
<div id="sec4" />
<p id="lv_0007" class="level2">Purposes of measurement and feedback</p>
<p class="indent">In QI, measurement and feedback are used to: (1) identify problems and establish baseline performance; (2) inform and guide QI projects; (3) select and test changes for improvement; and (4) assess progress toward organizational goals.</p>
<p class="indent">One strategy for evaluating performance and identifying potential areas for improvement is careful review of patient feedback from complaint systems, feedback forms, written and telephone surveys, and focus groups. Analysis of these data can help define gaps and create the case for conducting QI projects to address those gaps. Another mechanism for evaluating current performance is through continuous monitoring of system-level measures. For example, if performance gaps are detected in the hospital-wide infection rate, the leadership team can initiate a portfolio of improvement projects with the objective of improving this overall measure.</p>
<p class="indent">The second purpose of measurement in QI, which is the primary focus of this article, is to inform and guide QI projects. At the beginning of a QI project, a balanced set of measures, including outcome, process, and balancing measures, is established to support the team's aim statement (ie, the project goals).<a title="" href="#bib13" name="back-bib13"><sup>13</sup></a> These measures are then reported graphically (typically monthly) on run charts (see previous article on the model for improvement for an example of a run chart). Statistical process control charts<a title="" href="#bib14" name="back-bib14"><sup>14</sup></a> can also help monitor the progress of the project in accomplishing the improvement team's aim.</p>
<p class="indent">The third purpose of measurement and feedback is to develop, test, and implement changes. One common approach is to use PDSA cycles,<a title="" href="#bib13" name="back-bib13"><sup>13</sup></a> where each cycle is designed to answer specific questions about changes that the improvement team is testing. The “plan” step of the PDSA cycle involves specifying these questions and developing measures and a data collection plan to that will answer them. The previous article about the model for improvement shows several examples of PDSA cycles. Usually the measures used in these small tests of change are specific process measures related to the change(s) being evaluated in the cycle; sometimes, they are the project outcome measures stratified for the clinicians or patients who were part of the cycle. Often, the most important part of feedback in a PDSA cycle comes from qualitative data, particularly from comments staff or patients make about the changes being tested.</p>
<p class="indent">The fourth purpose of measurement is to assess progress toward organizational goals. The authors briefly discuss the use of organizational scorecards and dashboard in a later section of this article.</p>
<div id="sec5" />
<p id="lv_0008" class="level2">Optimal attributes of quality improvement measures</p>
<p class="indent">Selecting measures for QI projects can be challenging. Fortunately, there is increasing knowledge and experience to help improvement teams select useful measures. In this section, the authors describe the most important attributes of QI measures based on the literature and collective experience: tailored to the target audience, comprehensive, carefully defined, and involving minimal measurement burden. These attributes are summarized in <a title="" href="#tbl1" name="back-tbl1" class="ja50-ce-cross-ref">Table 1</a>.</p>
<div style="border: 1px solid #000000; padding: 5px; margin: 10px 0px ">
<p class="caption">
<b>Table 1</b>
<span class="caption">Key attributes of measures to support QI projects</span>
</p>
<table align="center" id="tbl1" border="1">
<colgroup>
<col />
<col />
</colgroup>
<thead>
<tr valign="top">
<th>Measure Attributes</th>
<th>Considerations</th>
</tr>
</thead>
<tbody>
<tr valign="top">
<td colspan="2">
<strong>Tailored to the target audience</strong>
</td>
</tr>
<tr valign="top">
<td>Meaningful, important and relevant to target audience</td>
<td>
<span class="ja50-ce-list">
<span class="list-item">
<span class="list-item-content">Address areas with substantial effect on the health of population</span>
</span>
<span class="list-item">
<span class="list-item-content">High burden of illness, high volume, problem-prone process, poor quality and/or high variation</span>
</span>
<span class="list-item">
<span class="list-item-content">Relevance to financial and strategic issues</span>
</span>
</span>
</td>
</tr>
<tr valign="top">
<td>Understandable to target audience</td>
<td>Avoid unnecessary complexity</td>
</tr>
<tr valign="top">
<td>Credible for target audience</td>
<td>
<span class="ja50-ce-list">
<span class="list-item">
<span class="list-item-content">Use nationally recognized practice guidelines when possible</span>
</span>
<span class="list-item">
<span class="list-item-content">If using nationally recognized practice guidelines, measures should account for patient preferences and clinician judgment</span>
</span>
</span>
</td>
</tr>
<tr valign="top">
<td colspan="2">
<strong>Comprehensive (ie, includes outcome, process, and balancing measures)</strong>
</td>
</tr>
<tr valign="top">
<td>Outcome measures</td>
<td>Outcome measures address how the health care services provided to patients affect their health, functional status, and/or satisfaction</td>
</tr>
<tr valign="top">
<td>Process measures</td>
<td>
<span class="ja50-ce-list">
<span class="list-item">
<span class="list-item-content">Process measures address the health care services provided to patients</span>
</span>
<span class="list-item">
<span class="list-item-content">Consider all or none measures</span>
</span>
</span>
</td>
</tr>
<tr valign="top">
<td>Balancing measures</td>
<td>Balancing measures address potential unintended consequences of changes to processes</td>
</tr>
<tr valign="top">
<td colspan="2">
<strong>Carefully defined</strong>
</td>
</tr>
<tr valign="top">
<td>Definition assures baseline levels are neither too high nor too low</td>
<td>
<span class="ja50-ce-list">
<span class="list-item">
<span class="list-item-content">If too high, difficult to show improvement</span>
</span>
<span class="list-item">
<span class="list-item-content">If too low, may be discouraging or cause disbelief</span>
</span>
</span>
</td>
</tr>
<tr valign="top">
<td>Definition assures measures are responsive to changes in the system</td>
<td>Minimize delay between improvements and measuring the effects of improvements</td>
</tr>
<tr valign="top">
<td>Definition minimizes the impact of variation</td>
<td>
<span class="ja50-ce-list">
<span class="list-item">
<span class="list-item-content">Reduce known causes of variation through stratification (eg, time of day, location/unit, staff or clinicians) to allow for more sensitivity in detecting improvement</span>
</span>
<span class="list-item">
<span class="list-item-content">Seasonal variation - avoid measures with this when possible or handle with rolling averages or year over year comparisons</span>
</span>
</span>
</td>
</tr>
<tr valign="top">
<td colspan="2">
<strong>Measurement burden minimized</strong>
</td>
</tr>
<tr valign="top">
<td>A small, balanced set of measures</td>
<td>
<span class="ja50-ce-list">
<span class="list-item">
<span class="list-item-content">Strive for a set of measures that describes a system as much as possible with as few measures as possible</span>
</span>
<span class="list-item">
<span class="list-item-content">Use or adapt existing measures when possible</span>
</span>
</span>
</td>
</tr>
<tr valign="top">
<td>Data collection built into the flow of work</td>
<td>
<span class="ja50-ce-list">
<span class="list-item">
<span class="list-item-content">Understand work flow (eg, through process mapping) and find best place to collect data in work flow and best person(s) to collect</span>
</span>
<span class="list-item">
<span class="list-item-content">Use existing data where possible</span>
</span>
</span>
</td>
</tr>
<tr valign="top">
<td>Small sample sizes</td>
<td>20–40 observations when collected frequently are often adequate (eg, medical record abstraction on 25 patients collected monthly)</td>
</tr>
<tr valign="top">
<td>Simple data collection instruments and methods</td>
<td>
<span class="ja50-ce-list">
<span class="list-item">
<span class="list-item-content">Use simple and quick instruments like check sheets, checklists</span>
</span>
<span class="list-item">
<span class="list-item-content">Leverage technology (eg, email surveys, scannable forms)</span>
</span>
</span>
</td>
</tr>
</tbody>
</table>
<p class="tbllgd">
<i>Data from</i> Refs. <a name="back-bib2" href="#bib2"><sup>2</sup></a><sup>, </sup><a name="back-bib16" href="#bib16"><sup>16</sup></a><sup>, </sup><a name="back-bib17" href="#bib17"><sup>17</sup></a><sup>, </sup><a name="back-bib22" href="#bib22"><sup>22</sup></a><sup>, </sup><a name="back-bib23" href="#bib23"><sup>23</sup></a>.</p>
</div>
<div id="sec5.1" />
<p id="lv_0009" class="level3">Tailoring to the Target Audience</p>
<p class="indent">When selecting measures, it is critical to consider the target audience (ie, those who will be viewing, using, and interpreting the data).<a name="back-bib15" href="#bib15"><sup>15–17</sup></a> The target audience will invariably include clinicians and clinical staff, so it is important that measures address high-impact clinical targets for the population of patients affected by the project (eg, using a common chronic condition like asthma for a chronic disease management improvement project).<a name="back-bib15" href="#bib15"><sup>15</sup></a><sup>, </sup><a name="back-bib16" href="#bib16"><sup>16</sup></a> The target audience should always include system leaders; therefore, measures should link to high-level organizational priorities such as strategic and financial goals.<a name="back-bib16" href="#bib16"><sup>16</sup></a><sup>, </sup><a name="back-bib17" href="#bib17"><sup>17</sup></a> Although there are some advantages to borrowing measures used by peer organizations, teams should be aware that some measures, especially those originating from research, can be overly complex or difficult to understand for the local target audience. An example is the Continuity of Care index (<img src="si1.gif" />) used in many pediatric continuity of care research studies.<a title="" href="#bib18" name="back-bib18"><sup>18</sup></a> Using a simpler measure, such as the percentage of patients seen by their primary care physician, would be much more intuitive for most clinicians.</p>
<p class="indent">Quality improvement measures must also be credible to the target audience. Measures from national organizations should be used when applicable, as long as they align well with the interests of the local target audience. Good resources for publicly available, evidence-based QI measures across a wide variety of clinical topics include the National Quality Measures Clearinghouse sponsored by the Agency for Healthcare Research and Quality,<a title="" href="#bib16" name="back-bib16"><sup>16</sup></a> the National Quality Forum,<a title="" href="#bib19" name="back-bib19"><sup>19</sup></a> the Institute for Healthcare Improvement,<a title="" href="#bib20" name="back-bib20"><sup>20</sup></a> and the National Initiative for Children's Healthcare Quality.<a title="" href="#bib21" name="back-bib21"><sup>21</sup></a> However, it is worth noting that the availability of pediatric-specific national measures lags that of adult-specific measures.</p>
<div id="sec5.2" />
<p id="lv_0010" class="level3">Including Comprehensive Measures</p>
<p class="indent">Three types of measures are essential to QI: (1) outcome measures, or those that address how the health care services provided to patients affect their health, functional status, and/or satisfaction; (2) process measures, which address the health care services provided to patients; and (3) balancing measures, which evaluate unintended consequences or the stability of the system being changed in the project. A balanced set of measures for a QI effort should include at least one outcome, process, and balancing measure.<a name="back-bib16" href="#bib16"><sup>16</sup></a><sup>, </sup><a name="back-bib17" href="#bib17"><sup>17</sup></a><sup>, </sup><a name="back-bib22" href="#bib22"><sup>22</sup></a></p>
<p class="indent">Outcome measures are significant for clinicians, as well as for leaders, who want to know the ultimate impact of a project. However, these measures can be slow to change over time, so inclusion of one or more process measures allows the team to see the effects of a QI effort more quickly. In addition, at the end of the project, process measures demonstrate that the intended changes were indeed implemented.</p>
<p class="indent">Process measures also illustrate the link between the changes made to the system and changes in the outcome. A common approach to calculating improvement is by an item-by-item measurement, such as the proportion of patients who receive each individual component of a health care service. Another approach, all-or-none measurement, is potentially advantageous in some situations. In this approach, the numerator is the number of patients who receive all of the measured components and the denominator is the total number of patients. All-or-none measures are ideal for a process that includes a series of critical steps, all of which must be completed to produce desirable outcomes, such as the process of inserting a central line.<a title="" href="#bib23" name="back-bib23"><sup>23</sup></a> If all-or-none measurement is not appropriate, other types of composite measurement can be used.<a title="" href="#bib24" name="back-bib24"><sup>24</sup></a></p>
<p class="indent">Any improvement project, which by definition involves making changes to one or more processes, can have unintended consequences. Balancing measures assess these potential unintended consequences and assure teams that they have indeed improved their overall system, rather than optimizing one part of the system at the expense of another.<a title="" href="#bib17" name="back-bib17"><sup>17</sup></a> Balancing measures can also be important in helping address the concerns of those who are resistant to the proposed changes. For example, in a project to improve immunization delivery in a private pediatric practice, a balancing measure might be the effect of changes to this high-volume process on office visit wait times.</p>
<div id="sec5.3" />
<p id="lv_0011" class="level3">Carefully Defining Measures</p>
<p class="indent">It is important to define measures in such a way that the baseline levels (ie, the level of performance before the QI project starts) are neither too high (difficult to detect improvement) nor too low (may be deflating or seem implausible to the target audience). Measures must also be responsive (show improvement in a timely fashion) to the planned changes in the system, and minimize the impact of variation unrelated to process changes, thereby improving the “signal-to-noise ratio” for detecting improvements to the system.<a name="back-bib23" href="#bib23"><sup>23</sup></a><sup>, </sup><a name="back-bib25" href="#bib25"><sup>25</sup></a> For example, a difference in wait times on Mondays versus Wednesdays related to differences in patient volumes on those days would be “noise” in an initiative attempting to lower overall wait times.</p>
<p class="indent">During a QI project, a measure's responsiveness to detecting change is affected by the time between piloting a change on a small scale and having the change fully implemented, as well as the time to have the change affect a patient's care and the time to sampling the affected patients. The last issue is particularly important in pediatrics because researchers often sample by age group (eg, sampling 24-month to 30-month olds for immunization status for vaccines given between 0 to 15 months). Thus, months may elapse between the introduction of changes and the measurement of their effects. Optimizing how a measure is defined and sampled can often minimize these delays. For example, changing a measure's denominator to “all children seen in the past month with asthma” rather than “all children seen in the past quarter with asthma” would allow for more rapid detection of the results of a change to asthma care processes.</p>
<div id="sec5.4" />
<p id="lv_0012" class="level3">Minimizing Measurement Burden</p>
<p class="indent">Finally, and most importantly, teams should minimize the burden of measurement for their QI project.<a name="back-bib17" href="#bib17"><sup>17</sup></a><sup>, </sup><a name="back-bib25" href="#bib25"><sup>25</sup></a><sup>, </sup><a name="back-bib26" href="#bib26"><sup>26</sup></a> Effort spent creating, collecting, displaying, analyzing, and interpreting data, although absolutely critical to improvement, is nonetheless time taken away from making changes to the system. The measurement burden is much like the burden of carrying water on a hike up a mountain: hikers must carry water on the way to the top, but carrying too much water will slow them down unnecessarily, or worse, tire them to the point of not being able to reach the top at all. A small, balanced set of five to eight measures will usually suffice for most improvement projects. <a title="" href="#tbl1" name="back-tbl1" class="ja50-ce-cross-ref">Table 1</a> lists several strategies to minimize the burden of measurement, such as using data that already exist in your organization.</p>
<div id="sec5.5" />
<p id="lv_0013" class="level3">Putting it All Together: A Balancing Act</p>
<p class="indent">Measurement selection involves making tradeoffs among these desirable measure attributes. For example, a measure that is more responsive to changes in a system, such as measuring immunizations by 18 months rather than by 24 months, may have a very low baseline level in a particular practice. Improvement teams are well served if they are aware of these attributes and can weigh these tradeoffs as they strive to create useful, though never perfect, measures for their project. <a title="" href="#tbl2" name="back-tbl2" class="ja50-ce-cross-ref">Table 2</a> presents an example of one project team's measure set to illustrate a product of this balancing act.</p>
<div style="border: 1px solid #000000; padding: 5px; margin: 10px 0px ">
<p class="caption">
<b>Table 2</b>
<span class="caption">Example of project measurement set used by the UNC Division of Pediatric Gastroenterology for improving care for children with inflammatory bowel diseases</span>
</p>
<table align="center" id="tbl2" border="1">
<colgroup>
<col />
<col />
</colgroup>
<thead>
<tr valign="top">
<th>Measure</th>
<th>Project Goal</th>
</tr>
</thead>
<tbody>
<tr valign="top">
<td colspan="2">Process measures</td>
</tr>
<tr valign="top">
<td>% patients seen this month with disease activity recorded</td>
<td align="char" char=".">95%</td>
</tr>
<tr valign="top">
<td>% patients seen this month with active disease that have a documented plan to escalate therapy</td>
<td align="char" char=".">95%</td>
</tr>
<tr valign="top">
<td>% patients seen this month with classified steroid status</td>
<td align="char" char=".">95%</td>
</tr>
<tr valign="top">
<td>% chronic steroid users (per clinician) seen this month with documented plan for tapering (± maintenance medication)</td>
<td align="char" char=".">95%</td>
</tr>
<tr valign="top">
<td>% patients seen this month with growth and nutritional status classified at visit</td>
<td align="char" char=".">95%</td>
</tr>
<tr valign="top">
<td>% patients seen this month classified as “at risk” or “failure” for nutrition with documented intervention</td>
<td align="char" char=".">95%</td>
</tr>
<tr valign="top">
<td colspan="2">Outcome measures</td>
</tr>
<tr valign="top">
<td>% of patients with active disease (mild, moderate, severe) measured by physician global assessment this month</td>
<td align="char" char=".">&lt; 20%</td>
</tr>
<tr valign="top">
<td>% patients classified as satisfactory growth status</td>
<td align="char" char=".">95%</td>
</tr>
<tr valign="top">
<td colspan="2">Balancing measure</td>
</tr>
<tr valign="top">
<td>Waiting time</td>
<td>N/A</td>
</tr>
</tbody>
</table>
</div>
<div id="sec6" />
<p id="lv_0014" class="level2">The measurement development process</p>
<p class="indent">After identifying robust candidate measures with the attributes described above, it is time to commence the measurement development process. The process of measurement development involves establishing operational definitions, collecting data, pilot testing, establishing baselines, and setting goals. During this process, it is also important to plan for sustainability after the project is complete. The next section describes the measurement development process in detail.</p>
<div id="sec6.1" />
<p id="lv_0015" class="level3">Establishing Operational Definitions</p>
<p class="indent">The first step of the measurement development is establishing operational definitions. Well-intentioned measures often fail to yield actionable information because of a lack of clear operational definitions.<a title="" href="#bib6" name="back-bib6"><sup>6</sup></a> For example, a QI team aiming to improve the emergency department (ED) care of children with pneumonia might select the measure: “Percentage of children with pneumonia who receive their first dose of antibiotics within four hours of arrival to the ED.” Operational definitions for this measure would need to include, at a minimum, how target children are defined (eg, age restrictions, chronic disease exclusions); how the presence of pneumonia is defined (eg, clinical versus radiologic versus administrative [eg, billing] diagnosis); and how children are accounted for if they received antibiotics before their arrival in the ED or left the ED in less than four hours.</p>
<p class="indent">When defining measures, many QI teams have found it helpful to create a table that summarizes the relevant features of each measure. Measure name, numerator, denominator, source, frequency of collection, and inclusion and exclusion criteria are a minimal set of features, but more detailed specification is often necessary. In addition to defining individual measures, it is important to consider whether to track measures independently (eg, whether each of five steps in a ventilator care bundle were performed daily) or in an all-or-none fashion (eg, whether all five steps in the ventilator care bundle were performed daily), as stated earlier.<a title="" href="#bib10" name="back-bib10"><sup>10</sup></a></p>
<div id="sec6.2" />
<p id="lv_0016" class="level3">Collecting Data</p>
<p class="indent">A critical step in developing a data collection strategy is careful consideration of the utility and accessibility of existing data sources. In many health care settings, the electronic medical record (EMR) serves as a valuable source of data. For example, an immunization improvement effort might involve querying the EMR monthly to assess the immunization status for all children of a certain age. These electronic systems are often a robust source of data, but they can be difficult to access. Using an EMR can be a challenge unless improvement team members have both administrative access to the data and the technological expertise required to extract and analyze the data.</p>
<p class="indent">Another strategy involves analyzing data collected in an existing patient registry. For example, patient registries of children with chronic health conditions can be used to monitor performance on outcome, process, and even balancing measures. One advantage of registry data is that they typically provide information on the entire population of interest (eg, all children with sickle cell disease seen in a hematology clinic). A disadvantage of registry data is that data specifications are typically already in place before the initiation of an improvement project, limiting the scope of what can be tracked to what already exists in the registry.</p>
<p class="indent">Regardless of the means of data collection, it is essential to develop a collection strategy that yields a maximal quality of actionable data with a minimal amount of extra effort. Data collected for other purposes (eg, clinic billing) can often be used in improvement efforts, and may involve substantially less time and effort to collect than data collected specifically for a QI project. Nonetheless, it is often worth comparing a small sample of billing data with more robust data sources (eg, EMR) to ensure that billing data accurately reflect the results they are intended to measure. For example, a review of 20 visits for which the billing data indicate a principal diagnosis code of bronchiolitis could be compared with EMR notes from each of the 20 visits to confirm the accuracy of the billing data as a means of identifying bronchiolitis visits.</p>
<div id="sec6.3" />
<p id="lv_0017" class="level3">Pilot Testing</p>
<p class="indent">Measures that appear conceptually strong often fail in the implementation process. For example, using a validated questionnaire is a logical approach to measuring and tracking patient satisfaction; however, if patient flow in the setting of interest is such that patients rarely have time to complete the survey, data collection will be problematic. By pilot testing measures and the data collection process on a small scale, improvement teams can determine whether the measurement system functions as planned. Shortening the survey, administering it at a different point in the visit, conducting it as an interview instead of a survey, or mailing it to patients might all be reasonable alternatives. These strategies would also need to be pilot tested to directly observe potential barriers to their effectiveness. Often, correctable errors (eg, “which provider did you see today?” inadvertently omitted from the patient satisfaction survey) can be identified after pilot testing the survey with a few patients.</p>
<div id="sec6.4" />
<p id="lv_0018" class="level3">Establishing Baselines and Setting Goals</p>
<p class="indent">Establishing baseline performance and setting performance goals are essential steps in developing the measurement strategy. In simple terms, measuring baseline performance answers the question, “where are we now?” and setting a goal establishes “where do we want to go and how soon do we want to get there?” The period of baseline measurement should be long enough to provide convincing data to recipients but short enough to avoid impeding initiation of QI efforts.</p>
<p class="indent">In the authors' experience, QI efforts are frequently derailed by attempts to collect “perfect” baseline data when more concise (or approximate) data would suffice. Depending on the frequency of the events being measured, prospective versus retrospective baseline data collection may be considered. For example, in a project aimed at reducing catheter-associated bloodstream infections in a pediatric intensive care unit (ICU), historical (ie, retrospective) data from the preceding year may be the best approach for establishing an accurate baseline, given the rarity of the event. In contrast, a 1-week, prospective hand-washing observation period may provide sufficient baseline data for a hand-hygiene QI project.</p>
<p class="indent">Goal setting is typically focused on process and outcome measures and can be conceptualized in several ways. First, QI teams can set absolute goals (eg, “90% receiving a preventive service”), which may be consistent with external standards (such as Healthy People 2010). Absolute goals may help team members see linkages between their local efforts and larger regional and national efforts to improve care, as well as compare their performance directly against a standardized benchmark. Some teams choose to set across-the-board, though arbitrary, absolute goals for their process measures (eg, “95% completion of all processes”) while linking absolute goals for outcome measures to national benchmarks. Second, teams can set goals relative to baseline performance (eg, “reduce the time required to process refill prescription requests by 50%”). This can be particularly effective when baseline performance is poor. When the outcomes being measured are rare (eg, “pediatric codes outside the ICU”), goals can be conceptualized in terms of time between events (eg, “reach 6 months with no pediatric codes outside the ICU”). Finally, goals can be a combination of the above strategies, as well as staged over time (eg, “reduce wait time for well child appointments to half of present wait time within 6 months, and to less than 2 days within 12 months”). In the authors' experience, whether they are staged or not, it is usually preferable to use absolute goals rather than relative goals because they are more tangible and meaningful to teams and stakeholders.</p>
<div id="sec6.5" />
<p id="lv_0019" class="level3">Planning for Sustainability</p>
<p class="indent">Sustainability is a key component of measurement development, and should be considered from the earliest phases. Quality improvement efforts that are sustained over long periods are usually those in which sustainability is considered early in the program's design and implementation, including the measurement strategy. What may seem like a reasonable measurement strategy when viewed in the lifespan of a QI project (eg, “review 10 medical records each week to determine the percentage of children receiving recommended hemoglobin screening”) may seem burdensome when extended beyond the project period. If data collection is considered “extra” work to the involved staff or is performed by an external research team or a team that will not operate at the completion the project, long-term sustainability of the measurement process will be unlikely. Conversely, if data collection is integrated into the everyday responsibilities of frontline staff, the likelihood of sustained data collection will increase. Successful teams often reduce the frequency of measurement (eg, from weekly to monthly or monthly to quarterly) after goal performance has been reached, as well as strive to “hard-wire” data collection into standard practice operations. The authors provide further discussion about implementing the sustainability plan later in this article.</p>
<div id="sec7" />
<p id="lv_0020" class="level2">Principles of quality improvement feedback</p>
<p class="indent">Quality improvement measures not only document progress in QI efforts but also serve as strong motivators for improvement. However, in order for these data to resonate with and motivate frontline clinicians and staff, both the message and the messenger must be thoughtfully considered. Bradley and colleagues<a title="" href="#bib27" name="back-bib27"><sup>27</sup></a> recently identified seven themes essential to effective feedback of data in QI projects (Box 1). Six of the seven themes relate directly to the message of the data feedback, and the remaining theme relates to the messenger.</p>
<div class="ja50-ce-textbox" id="tbox1">
<div class="ja50-ce-textbox-label">Box 1</div>
<div class="ja50-ce-textbox-caption">
<p class="normal">Key themes for effective quality improvement measurement feedback</p>
</div>
<div class="ja50-ce-textbox-body">
<p id="lv_0021" class="level1">
<p class="indent1">
<span class="ja50-ce-list">
<span class="list-item">
<span class="list-item-content">Data must be perceived by [clinicians] as valid to motivate change</span>
</span>
<span class="list-item">
<span class="list-item-content">It takes time to develop the credibility of data within [an organization]</span>
</span>
<span class="list-item">
<span class="list-item-content">The source and timeliness of data are critical to perceived validity</span>
</span>
<span class="list-item">
<span class="list-item-content">Benchmarking improves the meaningfulness of the data feedback</span>
</span>
<span class="list-item">
<span class="list-item-content">[Clinician] leaders can enhance the effectiveness of data feedback</span>
</span>
<span class="list-item">
<span class="list-item-content">Data feedback that profiles an individual [clinician's] practices can be effective but may be perceived as punitive</span>
</span>
<span class="list-item">
<span class="list-item-content">Data feedback must persist to sustain improved performance</span>
</span>
</span>
</p>
<p class="indent">
<i>Data from</i> Bradley EH, Holmboe ES, Mattera JA, et al. Data feedback efforts in quality improvement: lessons learned from US hospitals. Qual Saf Health Care 2004;13(1):26–31.</p>
</p>
</div>
</div>
<div id="sec7.1" />
<p id="lv_0022" class="level3">The Message: Is it Believable?</p>
<p class="indent">The importance of the perceived validity of the feedback is highlighted in three themes: (1) data must be viewed as valid to motivate change; (2) it takes time to develop data credibility; and (3) the source and timeliness of data feedback are critical to perceived validity. Discharge diagnosis codes, for example, can be an excellent means of identifying cases of a given illness in administrative data. However, if clinicians are skeptical about the accuracy of discharge diagnosis coding, inferences drawn from such data may garner little traction for improvement. Likewise, year-old data leave room for clinicians to assert, “things have changed since then,” whereas real-time data are more difficult to dismiss.</p>
<div id="sec7.2" />
<p id="lv_0023" class="level3">The Message: Is it in the Proper Context and can it be Sustained?</p>
<p class="indent">As previously discussed, feedback can be framed in ways that enhance its effectiveness. Comparison of local data to national or other benchmarks can provide motivation for improvement. Similarly, clinician-level data documenting each individual's performance in relation to the group's performance can be valuable, though considerable care is needed to avoid creating a punitive atmosphere. Lastly, just as the sustainability of any measurement strategy must be considered in its development, the sustainability of the feedback system is equally important and must be formally integrated into the standard operating procedure of an organization if it is to persist.</p>
<div id="sec7.3" />
<p id="lv_0024" class="level3">The Message: Does it Effectively Convey the Underlying Information?</p>
<p class="indent">Valid and actionable data are most effective when they are presented to frontline users in visually compelling ways.<a name="back-bib28" href="#bib28"><sup>28</sup></a><sup>, </sup><a name="back-bib29" href="#bib29"><sup>29</sup></a> A complex table of data may provide all the necessary information, but this display of data may make it difficult to determine whether improvements are being made. In contrast, an annotated run chart, which plots data over time, can more readily provide answers to questions such as, “how far have we progressed toward our goal?”, “how much further do we have to go?”, “have we sustained our gains?” and “what change, if any, resulted in an improvement?” Annotating run charts to indicate key events (eg, time the project team started or new change introduced) and applying statistical process control limits to distinguish random (noise) from nonrandom variation (signal) add useful detail to the visual display of performance data. These enhancements help distinguish true changes in processes or outcomes from the expected variation seen in stable systems over time.</p>
<div id="sec7.4" />
<p id="lv_0025" class="level3">The Messenger</p>
<p class="indent">Respected leaders, often described as “champions,” are frequently the most effective messengers for delivering performance data to frontline clinicians and staff.<a name="back-bib27" href="#bib27"><sup>27</sup></a><sup>, </sup><a name="back-bib30" href="#bib30"><sup>30</sup></a> For example, when physician leaders provide feedback to other physicians, it provides validation that the data came from colleagues with shared interests in promoting, as well as shared barriers to providing, optimal clinical care. In contrast, when data originate from an external body, such as a QI department or payer, clinicians may be more likely to view the data as invalid, punitive, or simply irrelevant. The importance of identifying messengers who can convey the feedback in honest and motivational ways cannot be overstated and is consistent with the literature on the effectiveness of academic detailing (the use of clinical expert outreach visits to physician practices).<a title="" href="#bib31" name="back-bib31"><sup>31</sup></a></p>
<div id="sec8" />
<p id="lv_0026" class="level2">Assuring sustainability of ongoing measurement</p>
<p class="indent">Achieving sustainability of QI projects depends on continuing to measure key processes and outcomes. For QI teams, measurement provides a source of learning during implementation and a method of maintenance after implementation. Some of the measures developed and used in testing and implementation should be considered for ongoing use in the organization when the QI team disbands. However, the measures may be reported and analyzed less frequently during this ongoing monitoring phase. Measuring over time allows an organization to determine whether it is continuing to get the desired results and whether those results can be predicted to continue in the future. This process addresses the question, ”Is there a need to update the process or make new changes?”</p>
<p class="indent">To prepare for this continued use of key measures, the QI team should consider strategies such as these:<span class="ja50-ce-list"><span class="list-item"><span class="ja50-ce-list-item-label">▪</span> <span class="list-item-content">Incorporate measurement with another existing work activity;</span></span><span class="list-item"><span class="ja50-ce-list-item-label">▪</span> <span class="list-item-content">Use existing data collection systems or develop easy-to-use data collection forms;</span></span><span class="list-item"><span class="ja50-ce-list-item-label">▪</span> <span class="list-item-content">Build measurement into EMRs, registries, or other data systems so that it is easy for administrative staff and clinicians to use as part of usual clinical care;</span></span><span class="list-item"><span class="ja50-ce-list-item-label">▪</span> <span class="list-item-content">Clearly define roles and responsibilities for ongoing data collection after the QI project is complete;</span></span><span class="list-item"><span class="ja50-ce-list-item-label">▪</span> <span class="list-item-content">Set aside time to review data with those who collect it before completion of the QI project. This will help everyone understand how the data is being used, and provide reinforcement for their efforts.</span></span></span></p>
<p class="indent">Although run charts of the key measures are effective for learning during an improvement project, plotting key measures on a statistical process control chart is more useful for maintaining a change. These charts provide signals to detect whether the process is beginning to deteriorate (eg, staff reverts to practices used before the improvements were made). In addition, control charts allow teams to predict future expected performance. Statistical process control charts can be maintained for the key measures used in any improvement effort. <a title="" href="#fig2" name="back-fig2" class="ja50-ce-cross-ref">Fig. 2</a> shows an example of a control chart<a title="" href="#bib32" name="back-bib32"><sup>32</sup></a> for a measure of harm during hospital admission. These maintenance charts will provide signals if the process ever begins to deteriorate, or, in this example, if data exceeds the upper control limit.</p>
<div style="border: 1px solid #000000; padding: 5px; margin: 10px 0px ">
<a href="gr2_lrg.jpg">
<p class="img">
<img src="gr2.jpg" alt="image" id="fig2" />
</p>
</a>
<p class="caption">
<b>Fig. 2</b>   <span class="caption">Statistical process control chart showing an improved process that can be monitored for sustainability. The chart has an annotation marking the beginning of the QI intervention.<br /></span></p>
</div>
<div id="sec9" />
<p id="lv_0027" class="level2">Measurement for supporting organizational performance</p>
<p class="indent">Just as measures are used to identify performance gaps and set priorities for improvement at the project level, health care organizations, including clinical practices, networks, hospitals, and health systems, also need a balanced set of system-level measures to track progress toward their strategic goals. Measures guide the direction and focus of QI efforts across the organization and should complement other system-level performance measures (eg, finance, use).<a title="" href="#bib33" name="back-bib33"><sup>33</sup></a> Collectively, this set of measures should serve as both a gauge of current performance and as input into the future direction of the organization.<a title="" href="#bib34" name="back-bib34"><sup>34</sup></a> Leaders and governing boards need to be actively involved in selecting and developing a dashboard of quality measures that reflects the culture and mission of their organization.<a name="back-bib30" href="#bib30"><sup>30</sup></a><sup>, </sup><a name="back-bib35" href="#bib35"><sup>35</sup></a> To improve quality at the system-level, senior leadership needs to be committed and accountable to improving (or maintaining) performance on these measures, and prepared to build the will needed to drive change throughout the organization.<a name="back-bib30" href="#bib30"><sup>30</sup></a><sup>, </sup><a name="back-bib36" href="#bib36"><sup>36</sup></a></p>
<p class="indent">Organizational performance measures should help leaders understand their progress toward accomplishing their mission. Leadership should identify a limited set of approximately 10 to 20 high-level measures that are focused on what the organization wants to accomplish and are balanced from the perspective of organization stakeholders. Dashboard measures should endure year to year; however, the strategic priorities of the organization will often focus on one or two measures each year.</p>
<p class="indent">This dashboard of measures should include operational definitions that can be easily understood at all levels of the organization, from the leadership to front-line clinicians and staff (see <a title="" href="#tbl3" name="back-tbl3" class="ja50-ce-cross-ref">Table 3</a> for an example of a children's hospital dashboard). Like project measurement, organizational measurement must be timely (no more than a month's lag between data and review) and it should not require excessive data collection, which hinders sustainability. The data should be extracted and graphed to show patterns and trends so that improvements can be tracked over time.<a name="back-bib30" href="#bib30"><sup>30</sup></a><sup>, </sup><a name="back-bib37" href="#bib37"><sup>37</sup></a> Following these principles ensures that leaders and the governing board of the health care system can continuously monitor and respond to data, and keep the organization moving toward meeting its goals.</p>
<div style="border: 1px solid #000000; padding: 5px; margin: 10px 0px ">
<p class="caption">
<b>Table 3</b>
<span class="caption">An example of a children's hospital dashboard, which tracks the organization's performance.</span>
</p>
<table align="center" id="tbl3" border="1">
<colgroup>
<col />
<col />
<col />
</colgroup>
<thead>
<tr valign="top">
<th>Strategic Category</th>
<th>Measure</th>
<th>Operational Definition</th>
</tr>
</thead>
<tbody>
<tr valign="top">
<td rowspan="3">People</td>
<td>New hire nursing turnover</td>
<td>% nurses leaving within 12 months of hire</td>
</tr>
<tr valign="top">
<td>Faculty overall satisfaction</td>
<td>% rating “very good”</td>
</tr>
<tr valign="top">
<td>Staff overall satisfaction</td>
<td>% rating “very good”</td>
</tr>
<tr valign="top">
<td>Service</td>
<td>Patient/family overall satisfaction</td>
<td>% recommending UNC</td>
</tr>
<tr valign="top">
<td rowspan="6">Clinical Quality</td>
<td>Adverse events</td>
<td>Adverse drug events per 1000 doses</td>
</tr>
<tr valign="top">
<td>Hospital associated infections</td>
<td># infections per 1000 pt days</td>
</tr>
<tr valign="top">
<td>Raw mortality</td>
<td># inpatient deaths</td>
</tr>
<tr valign="top">
<td>Cardiac arrests</td>
<td># cardiac arrests per 1000 pt days</td>
</tr>
<tr valign="top">
<td>Chronic care management</td>
<td># patients in chronic care registry</td>
</tr>
<tr valign="top">
<td>Readmission rate</td>
<td># patients readmitted within 72 hours</td>
</tr>
<tr valign="top">
<td rowspan="4">Finance</td>
<td>Payor mix</td>
<td>% distribution of gross revenues across insurance plans</td>
</tr>
<tr valign="top">
<td>Gross charges</td>
<td>Actual cash collection on bills</td>
</tr>
<tr valign="top">
<td>Fundraising</td>
<td>Amount of gifts per year</td>
</tr>
<tr valign="top">
<td>RVU</td>
<td># relative value units (inpatient and outpatient)</td>
</tr>
<tr valign="top">
<td rowspan="2">Innovation</td>
<td>Publications</td>
<td># faculty publications per quarter</td>
</tr>
<tr valign="top">
<td>Family advisory groups</td>
<td># active family advisory groups</td>
</tr>
<tr valign="top">
<td rowspan="4">Growth</td>
<td>Hours on diversion</td>
<td># hours PICU unable to accept new admissions</td>
</tr>
<tr valign="top">
<td>Inpatient admissions</td>
<td># inpatients admitted to wards, PICU and NICU</td>
</tr>
<tr valign="top">
<td>Inpatient patient days</td>
<td># inpatients in a bed</td>
</tr>
<tr valign="top">
<td>Inpatient length of stay</td>
<td>Average # days patients stay in hospital</td>
</tr>
</tbody>
</table>
<p class="tbllgd">
<i>Courtesy of</i> North Carolina Children's Hospital, Chapel Hill, NC; with permission.</p>
</div>
<p class="indent">Another question that system-level performance measures should help to answer is: “How do we compare with others?” Unlike the mission question stated above, which should be asked at every leadership and board meeting, this question should be addressed annually when reevaluating the organization's strategic goals.<a title="" href="#bib38" name="back-bib38"><sup>38</sup></a> Benchmarking against peer organizations can help determine how the system is comparatively performing and it can identify opportunities for learning from other best practices.<a title="" href="#bib37" name="back-bib37"><sup>37</sup></a></p>
<p class="indent">Tracking organizational performance on quality measures is increasingly relevant to the financial security of practices, networks, hospitals, and health systems. These measures are being incorporated into federal data reporting standards, such as The Joint Commission and the CMS Core Measures. CMS will link hospital reimbursement to organizations' performance on the core measures, a strategy known as “pay for performance,” an approach that numerous payors have adopted or will soon adopt.<a name="back-bib39" href="#bib39"><sup>39</sup></a><sup>, </sup><a name="back-bib40" href="#bib40"><sup>40</sup></a></p>
<div id="sec10" />
<p id="lv_0028" class="level2">Summary</p>
<p class="indent">Measurement and feedback are fundamental aspects of QI. The authors have described a pragmatic approach to measurement and feedback for QI efforts in local health care settings, including hospitals and clinical practices. The authors included evidence-based strategies from health care and other industries, augmented by their collective practical experience designing measurement and feedback strategies. The authors also described an approach to developing, testing, and implementing measurement and feedback strategies during the stages of a typical QI project.</p>
<p class="indent">The process as described here will assist health care professionals in knowing how to measure the effects of their QI projects and how to use these data to improve care. Health care professionals will need to understand and know how to apply the principles that are summarized in this article in order for hospitals and clinical practices to meet the growing demand to dramatically improve their performance.</p>
</p>
<p id="lv_0029" class="level2">Acknowledgments</p>
<p class="indent">The authors are grateful for the thoughtful review and feedback provided by John B. Anderson, MD, MPH; Virginia (Ginna) Crowe, RN, EdD; Michael Steiner, MD; Jayne M. Stuart, MPH; and Jane Taylor, EdD.</p>
<hr />
<div id="yoasNoAbstractFragment" />
<hr />
<p id="lv_0030" class="level2">References</p>
<p class="ref" id="bib1">
<a class="back-bib" href="#back-bib1">
<img src="UpArrow.png" />
</a> 1 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Institute of Medicine</span> <span class="ja50-ce-given-name"></span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Crossing the quality chasm: a new health system for the 21st century</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">Washington, DC: National Academy Press, </span><span class="ja50-sb-date">2001</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib2">
<a class="back-bib" href="#back-bib2">
<img src="UpArrow.png" />
</a> 2 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Nelson</span> <span class="ja50-ce-given-name">E.C.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Mohr</span> <span class="ja50-ce-given-name">J.J.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Batalden</span> <span class="ja50-ce-given-name">P.B.</span></span>, et al</span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Improving health care, part 1: the clinical value compass</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">Jt Comm J Qual Improv</span></span> <span class="ja50-sb-date">1996</span>; <span class="ja50-sb-volume-nr">22</span>: </span><span class="ja50-sb-pages">243-258</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib3">
<a class="back-bib" href="#back-bib3">
<img src="UpArrow.png" />
</a> 3 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Solberg</span> <span class="ja50-ce-given-name">L.L.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Mosser</span> <span class="ja50-ce-given-name">G.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">McDonald</span> <span class="ja50-ce-given-name">S.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">The three faces of performance measursmement: improvement, accountability and research</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">Jt Comm J Qual Improv</span></span> <span class="ja50-sb-date">1997</span>; <span class="ja50-sb-volume-nr">23</span> (<span class="ja50-sb-issue-nr">3</span>): </span><span class="ja50-sb-pages">135-147</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib4">
<a class="back-bib" href="#back-bib4">
<img src="UpArrow.png" />
</a> 4 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Codman</span> <span class="ja50-ce-given-name">E.A.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">A study in hospital efficiency (1917)</span></span></span>. <span class="ja50-sb-comment">Reprinted by the Joint Commission on Accreditation of Healthcare Organizations </span><span class="ja50-sb-host">. <span class="ja50-sb-publisher">Illinois: Oakbrook Terrace, </span><span class="ja50-sb-date">1996</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib5">
<a class="back-bib" href="#back-bib5">
<img src="UpArrow.png" />
</a> 5 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Shewhart</span> <span class="ja50-ce-given-name">W.A.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">The economic control of quality of manufactured product (1931)</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">Milwaukee (WI): Reprinted by ASQC, </span><span class="ja50-sb-date">1980</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib6">
<a class="back-bib" href="#back-bib6">
<img src="UpArrow.png" />
</a> 6 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Deming</span> <span class="ja50-ce-given-name">W.E.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Out of the crisis</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">Cambridge (MA): Massachusetts Institute of Technology, </span><span class="ja50-sb-date">1986</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib7">
<a class="back-bib" href="#back-bib7">
<img src="UpArrow.png" />
</a> 7 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Donabedian</span> <span class="ja50-ce-given-name">A.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Bashshur</span> <span class="ja50-ce-given-name">R.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">An introduction to quality assurance in health care</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">New York: Oxford University Press, </span><span class="ja50-sb-date">2003</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib8">
<a class="back-bib" href="#back-bib8">
<img src="UpArrow.png" />
</a> 8 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Kenney</span> <span class="ja50-ce-given-name">C.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">The best practice: how the new quality movement is transforming medicine</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">New York: Public Affairs, </span><span class="ja50-sb-date">2008</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib9">
<a class="back-bib" href="#back-bib9">
<img src="UpArrow.png" />
</a> 9 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Forrester</span> <span class="ja50-ce-given-name">J.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Principles of systems</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">Cambridge (MA): Productivity Press, </span><span class="ja50-sb-date">1986</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib10">
<a class="back-bib" href="#back-bib10">
<img src="UpArrow.png" />
</a> 10 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Nolan</span> <span class="ja50-ce-given-name">T.W.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Understanding medical systems</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">Annals of Internal Medicine</span></span> <span class="ja50-sb-date">1998</span>; <span class="ja50-sb-volume-nr">128</span> (<span class="ja50-sb-issue-nr">4</span>): </span><span class="ja50-sb-pages">293-298</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib11">
<a class="back-bib" href="#back-bib11">
<img src="UpArrow.png" />
</a> 11 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Deming</span> <span class="ja50-ce-given-name">W.E.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">The new economics for industry, government, and education</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">Cambridge (MA): Massachusetts Institute of Technology, </span><span class="ja50-sb-date">1994</span>. </span><span class="ja50-sb-comment">p. 92–115 </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib12">
<a class="back-bib" href="#back-bib12">
<img src="UpArrow.png" />
</a> 12 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Senge</span> <span class="ja50-ce-given-name">P.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">The fifth discipline: the art &amp; practice of the learning organization</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">New York: Doubleday, </span><span class="ja50-sb-date">1994</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib13">
<a class="back-bib" href="#back-bib13">
<img src="UpArrow.png" />
</a> 13 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Langley</span> <span class="ja50-ce-given-name">G.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Nolan</span> <span class="ja50-ce-given-name">K.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Nolan</span> <span class="ja50-ce-given-name">T.</span></span>, et al</span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">The improvement guide: a practical approach to enhancing organizational performance</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">San Francisco (CA): Jossey-Bass Pub, </span><span class="ja50-sb-date">1996</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib14">
<a class="back-bib" href="#back-bib14">
<img src="UpArrow.png" />
</a> 14 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Shewhart WA</span> <span class="ja50-ce-given-name"></span></span></span></span>. <span class="ja50-sb-host">In: <span class="ja50-sb-editors"><span class="ja50-sb-editor"><span class="ja50-ce-surname">Deming </span><span class="ja50-ce-given-name">W.E.</span></span></span>, ed. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Statistical method from the viewpoint of quality control (1939)</span></span>. <span class="ja50-sb-publisher">New York: Dover Press, </span><span class="ja50-sb-date">1986</span>: <span class="ja50-sb-pages">1-15</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib15">
<a class="back-bib" href="#back-bib15">
<img src="UpArrow.png" />
</a> 15 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Joint Commission on Accreditation of Health Care Organizations</span> <span class="ja50-ce-given-name"></span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Attributes of core performance measures and associated evaluation criteria</span></span></span>. <span class="ja50-sb-comment">Available at: </span><span class="ja50-sb-host"><a class="ja50-ce-inter-ref" href="http://www.jointcommission.org/PerformanceMeasurement/PerformanceMeasurement/" target="_blank">http://www.jointcommission.org/PerformanceMeasurement/PerformanceMeasurement/</a>. </span><span class="ja50-sb-comment">Accessed November 18, 2008 </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib16">
<a class="back-bib" href="#back-bib16">
<img src="UpArrow.png" />
</a> 16 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">NQMC Agency for Healthcare Research and Quality</span> <span class="ja50-ce-given-name"></span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">National quality measures clearinghouse</span></span></span>. <span class="ja50-sb-comment">Available at: </span><span class="ja50-sb-host"><a class="ja50-ce-inter-ref" href="http://www.qualitymeasures.ahrq.gov/" target="_blank">http://www.qualitymeasures.ahrq.gov/</a>. </span><span class="ja50-sb-comment">Accessed November 18, 2008 </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib17">
<a class="back-bib" href="#back-bib17">
<img src="UpArrow.png" />
</a> 17 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Association of Public Health Observatories</span> <span class="ja50-ce-given-name"></span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">The good indicators guide: understanding how to use and choose indicators. NHS Institute for Innovation and Improvement</span></span></span>. <span class="ja50-sb-comment">Available at: </span><span class="ja50-sb-host"><a class="ja50-ce-inter-ref" href="http://www.apho.org.uk/resource/item.aspx?RID=44584" target="_blank">http://www.apho.org.uk/resource/item.aspx?RID=44584</a>. </span><span class="ja50-sb-comment">Accessed November 18, 2008 </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib18">
<a class="back-bib" href="#back-bib18">
<img src="UpArrow.png" />
</a> 18 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Christakis</span> <span class="ja50-ce-given-name">D.A.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Wright</span> <span class="ja50-ce-given-name">J.A.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Zimmerman</span> <span class="ja50-ce-given-name">F.J.</span></span>, et al</span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Continuity of care is associated with well-coordinated care</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">Ambul Pediatr</span></span> <span class="ja50-sb-date">2003</span>; <span class="ja50-sb-volume-nr">3</span> (<span class="ja50-sb-issue-nr">2</span>): </span><span class="ja50-sb-pages">82-86</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib19">
<a class="back-bib" href="#back-bib19">
<img src="UpArrow.png" />
</a> 19 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">National Quality Forum</span> <span class="ja50-ce-given-name"></span></span></span></span>. <span class="ja50-sb-comment">Available at: </span><span class="ja50-sb-host"><a class="ja50-ce-inter-ref" href="http://www.qualityforum.org/" target="_blank">http://www.qualityforum.org/</a>. </span><span class="ja50-sb-comment">Accessed November 18, 2008 </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib20">
<a class="back-bib" href="#back-bib20">
<img src="UpArrow.png" />
</a> 20 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Institute for Healthcare Improvement</span> <span class="ja50-ce-given-name"></span></span></span></span>. <span class="ja50-sb-comment">Available at: </span><span class="ja50-sb-host"><a class="ja50-ce-inter-ref" href="http://www.ihi.org" target="_blank">http://www.ihi.org</a>. </span><span class="ja50-sb-comment">Accessed November 18, 2008 </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib21">
<a class="back-bib" href="#back-bib21">
<img src="UpArrow.png" />
</a> 21 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">National Initiative for Children's Healthcare Quality</span> <span class="ja50-ce-given-name"></span></span></span></span>. <span class="ja50-sb-comment">Available at: </span><span class="ja50-sb-host"><a class="ja50-ce-inter-ref" href="http://www.nichq.org" target="_blank">http://www.nichq.org</a>. </span><span class="ja50-sb-comment">Accessed November 18, 2008 </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib22">
<a class="back-bib" href="#back-bib22">
<img src="UpArrow.png" />
</a> 22 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">American Academy of Pediatrics Steering Committee on Quality Improvement and Management and Committee on Practice and Ambulatory Medicine</span> <span class="ja50-ce-given-name"></span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Principles for the development and use of quality measures. Policy statement</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">Pediatrics</span></span> <span class="ja50-sb-date">2008</span>; <span class="ja50-sb-volume-nr">121</span>: </span><span class="ja50-sb-pages">411-418</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib23">
<a class="back-bib" href="#back-bib23">
<img src="UpArrow.png" />
</a> 23 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Nolan</span> <span class="ja50-ce-given-name">T.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Berwick</span> <span class="ja50-ce-given-name">D.M.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">All-or-none measurement raises the bar on performance</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">JAMA</span></span> <span class="ja50-sb-date">2006</span>; <span class="ja50-sb-volume-nr">295</span> (<span class="ja50-sb-issue-nr">10</span>): </span><span class="ja50-sb-pages">1168-1170</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib24">
<a class="back-bib" href="#back-bib24">
<img src="UpArrow.png" />
</a> 24 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Reeves</span> <span class="ja50-ce-given-name">D.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Campbell</span> <span class="ja50-ce-given-name">S.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Adams</span> <span class="ja50-ce-given-name">J.</span></span>, et al</span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Combining multiple indicators of clinical quality: an evaluation of different analytic approaches</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">Med Care</span></span> <span class="ja50-sb-date">2007</span>; <span class="ja50-sb-volume-nr">45</span> (<span class="ja50-sb-issue-nr">6</span>): </span><span class="ja50-sb-pages">489-496</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib25">
<a class="back-bib" href="#back-bib25">
<img src="UpArrow.png" />
</a> 25 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Nelson</span> <span class="ja50-ce-given-name">E.C.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Splaine</span> <span class="ja50-ce-given-name">M.E.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Plume</span> <span class="ja50-ce-given-name">S.K.</span></span>, et al</span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Good measurement for good improvement work</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">Qual Manag Health Care</span></span> <span class="ja50-sb-date">2004</span>; <span class="ja50-sb-volume-nr">13</span> (<span class="ja50-sb-issue-nr">1</span>): </span><span class="ja50-sb-pages">1-16</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib26">
<a class="back-bib" href="#back-bib26">
<img src="UpArrow.png" />
</a> 26 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Pronovost</span> <span class="ja50-ce-given-name">P.J.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Nolan</span> <span class="ja50-ce-given-name">T.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Zeger</span> <span class="ja50-ce-given-name">S.</span></span>, et al</span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">How can clinicians measure safety and quality in acute care?</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">Lancet</span></span> <span class="ja50-sb-date">2004</span>; <span class="ja50-sb-volume-nr">363</span>: </span><span class="ja50-sb-pages">1061-1067</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib27">
<a class="back-bib" href="#back-bib27">
<img src="UpArrow.png" />
</a> 27 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Bradley</span> <span class="ja50-ce-given-name">E.H.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Holmboe</span> <span class="ja50-ce-given-name">E.S.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Mattera</span> <span class="ja50-ce-given-name">J.A.</span></span>, et al</span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Data feedback efforts in quality improvement: lessons learned from US hospitals</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">Qual Saf Health Care</span></span> <span class="ja50-sb-date">2004</span>; <span class="ja50-sb-volume-nr">13</span> (<span class="ja50-sb-issue-nr">1</span>): </span><span class="ja50-sb-pages">26-31</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib28">
<a class="back-bib" href="#back-bib28">
<img src="UpArrow.png" />
</a> 28 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Carey</span> <span class="ja50-ce-given-name">R.G.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Lloyd</span> <span class="ja50-ce-given-name">R.C.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Measuring quality improvement in healthcare: a guide to statistical process control application</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">Wisconsin: Quality Press, </span><span class="ja50-sb-date">2001</span>. </span><span class="ja50-sb-comment">p. 43–150 </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib29">
<a class="back-bib" href="#back-bib29">
<img src="UpArrow.png" />
</a> 29 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Tufte</span> <span class="ja50-ce-given-name">E.R.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">The visual display of quantitative information</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">Cheshire (CT): Graphics Press, </span><span class="ja50-sb-date">2001</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib30">
<a class="back-bib" href="#back-bib30">
<img src="UpArrow.png" />
</a> 30 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Denham</span> <span class="ja50-ce-given-name">C.R.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Leaders need dashboards, dashboards need leaders</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">J Patient Saf</span></span> <span class="ja50-sb-date">2006</span>; <span class="ja50-sb-volume-nr">2</span> (<span class="ja50-sb-issue-nr">1</span>): </span><span class="ja50-sb-pages">45-53</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib31">
<a class="back-bib" href="#back-bib31">
<img src="UpArrow.png" />
</a> 31 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">O'Brien</span> <span class="ja50-ce-given-name">M.A.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Rogers</span> <span class="ja50-ce-given-name">S.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Jamtvedt</span> <span class="ja50-ce-given-name">G.</span></span>, et al</span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Educational outreach visits: effects on professional practice and health care outcomes</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">Cochrane Database Syst Rev</span></span> <span class="ja50-sb-date">2007</span> (<span class="ja50-sb-issue-nr">4</span>)</span>. </span><span class="ja50-sb-comment">CD000409. DOI: 10.1002/14651858 </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib32">
<a class="back-bib" href="#back-bib32">
<img src="UpArrow.png" />
</a> 32 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Lloyd</span> <span class="ja50-ce-given-name">R.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Quality health care: a guide to developing and using indicators</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">Boston: Jones &amp; Bartlett Publishers, </span><span class="ja50-sb-date">2004</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib33">
<a class="back-bib" href="#back-bib33">
<img src="UpArrow.png" />
</a> 33 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Martin</span> <span class="ja50-ce-given-name">L.A.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Nelson</span> <span class="ja50-ce-given-name">E.C.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Lloyd</span> <span class="ja50-ce-given-name">R.C.</span></span>, et al</span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Whole system measures. IHI innovation series white paper</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">Cambridge (MA): Institute for Healthcare Improvement, </span><span class="ja50-sb-date">2007</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib34">
<a class="back-bib" href="#back-bib34">
<img src="UpArrow.png" />
</a> 34 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Provost</span> <span class="ja50-ce-given-name">L.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Leddick</span> <span class="ja50-ce-given-name">S.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">How to take multiple measures to get a complete picture of organizational performance</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">Natl Prod Rev</span></span> <span class="ja50-sb-date">1993</span>; <span class="ja50-sb-volume-nr">12</span> (<span class="ja50-sb-issue-nr">4</span>): </span><span class="ja50-sb-pages">477-490</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib35">
<a class="back-bib" href="#back-bib35">
<img src="UpArrow.png" />
</a> 35 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Kroch</span> <span class="ja50-ce-given-name">E.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Vaughn</span> <span class="ja50-ce-given-name">T.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Koepke</span> <span class="ja50-ce-given-name">M.</span></span>, et al</span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Hospital boards and quality dashboards</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">J Patient Saf</span></span> <span class="ja50-sb-date">2006</span>; <span class="ja50-sb-volume-nr">2</span> (<span class="ja50-sb-issue-nr">1</span>): </span><span class="ja50-sb-pages">10-19</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib36">
<a class="back-bib" href="#back-bib36">
<img src="UpArrow.png" />
</a> 36 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Conway</span> <span class="ja50-ce-given-name">J.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Getting boards on board: engaging governing boards in quality and safety</span></span></span>. <span class="ja50-sb-host"><span class="ja50-sb-issue"><span class="ja50-sb-title"><span class="ja50-sb-maintitle">Jt Comm J Qual Patient Saf</span></span> <span class="ja50-sb-date">2008</span>; <span class="ja50-sb-volume-nr">34</span> (<span class="ja50-sb-issue-nr">4</span>): </span><span class="ja50-sb-pages">214-220</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib37">
<a class="back-bib" href="#back-bib37">
<img src="UpArrow.png" />
</a> 37 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Reinertsen</span> <span class="ja50-ce-given-name">J.L.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Pugh</span> <span class="ja50-ce-given-name">M.D.</span>, </span><span class="ja50-sb-author"><span class="ja50-ce-surname">Bisognano</span> <span class="ja50-ce-given-name">M.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Institute for Healthcare Improvement. IHI innovation series. Seven leadership leverage points for organization-level improvement in health care</span></span></span>. <span class="ja50-sb-host">. <span class="ja50-sb-publisher">Cambridge (MA): Institute for Healthcare Improvement, </span><span class="ja50-sb-date">2005</span>. </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib38">
<a class="back-bib" href="#back-bib38">
<img src="UpArrow.png" />
</a> 38 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Reinertsen</span> <span class="ja50-ce-given-name">J.L.</span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">From the top: getting the board on board. IHI conference: boards, dashboards, and data. Boston</span></span></span>. <span class="ja50-sb-comment">Available at: </span><span class="ja50-sb-host"><a class="ja50-ce-inter-ref" href="http://www.ihi.org/IHI/Topics/LeadingSystemImprovement/Leadership/EmergingContent/BoardsDashboardsData.htm" target="_blank">http://www.ihi.org/IHI/Topics/LeadingSystemImprovement/Leadership/EmergingContent/BoardsDashboardsData.htm</a>. </span><span class="ja50-sb-comment">Accessed November 10, 2008 </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib39">
<a class="back-bib" href="#back-bib39">
<img src="UpArrow.png" />
</a> 39 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">The Joint Commission</span> <span class="ja50-ce-given-name"></span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Performance measurement initiatives</span></span></span>. <span class="ja50-sb-comment">Available at: </span><span class="ja50-sb-host"><a class="ja50-ce-inter-ref" href="http://www.jointcommission.org/PerformanceMeasurement/PerformanceMeasurement/default.htm" target="_blank">http://www.jointcommission.org/PerformanceMeasurement/PerformanceMeasurement/default.htm</a>. </span><span class="ja50-sb-comment">Accessed November 10, 2008 </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p class="ref" id="bib40">
<a class="back-bib" href="#back-bib40">
<img src="UpArrow.png" />
</a> 40 <span class="ja50-sb-contribution"><span class="ja50-sb-authors"><span class="ja50-sb-author"><span class="ja50-ce-surname">Kaiser Daily Health Policy Report</span> <span class="ja50-ce-given-name"></span></span></span>. <span class="ja50-sb-title"><span class="ja50-sb-maintitle">Medicare stops paying for 10 reasonably preventable medical errors</span></span></span>. <span class="ja50-sb-comment">Available at: </span><span class="ja50-sb-host"><a class="ja50-ce-inter-ref" href="http://www.kaisernetwork.org/daily_reports/rep_index.cfm?hint=3&amp;;DR_ID=54758" target="_blank">http://www.kaisernetwork.org/daily_reports/rep_index.cfm?hint=3&amp;;DR_ID=54758</a>. </span><span class="ja50-sb-comment">Accessed October 1, 2008 </span><a href="http://dx.doi.org/10.1016/j.pcl.2009.05.012" target="_blank">
<img src="LeftArrow.png" /></a></p>
<p id="lv_0031" class="level2">Glossary</p>
<div class="ja50-ce-glossary-entry">
<span class="ja50-ce-glossary-heading">Balanced set of measures</span>
:
A set of measures which, taken together, reflect as much of a system as possible without duplication, overlap or gaps.<a title="" href="#bib17" name="back-bib17"><sup>17</sup></a></div>
<div class="ja50-ce-glossary-entry">
<span class="ja50-ce-glossary-heading">Benchmark</span>
:
An externally agreed-upon comparator to compare performance between similar organizations or systems.<a title="" href="#bib17" name="back-bib17"><sup>17</sup></a></div>
<div class="ja50-ce-glossary-entry">
<span class="ja50-ce-glossary-heading">Composite indicator</span>
:
An aggregation of numerous indicators that aims to give a one-figure indicator in order to summarize measures further.</div>
<div class="ja50-ce-glossary-entry">
<span class="ja50-ce-glossary-heading">Control charts</span>
:
A graphical tool for displaying the results of statistical process control.</div>
<div class="ja50-ce-glossary-entry">
<span class="ja50-ce-glossary-heading">Control limits</span>
:
Define the area representing random (also called “common cause”) variation on either side of the centerline, plotted on a control chart.</div>
<div class="ja50-ce-glossary-entry">
<span class="ja50-ce-glossary-heading">Dashboard</span>
:
A tool used for collecting and reporting data on system-level measures that demonstrate the overall quality of a health system over time. Dashboards provide a quick summary of structural, process and outcome performance.</div>
<div class="ja50-ce-glossary-entry">
<span class="ja50-ce-glossary-heading">Plan-Do-Study-Act (PDSA) Cycle</span>
:
A quality improvement method consisting of four repetitive steps for learning and improvement. Plan: develop a plan to learn, test, or implement a change; Do: execute the plan; Study: compare predictions to results and document learning; Act: make changes based on learning and set up next cycle. Most changes require several PDSA cycles.</div>
<div class="ja50-ce-glossary-entry">
<span class="ja50-ce-glossary-heading">Run chart</span>
:
A time series graph where the X-axis represents time longitudinally and the measure value is on the Y-axis. Run charts often include a median for the data points and can be augmented by inserting comments (annotations) at the point in time where process changes are made by the improvement team or other changes occur that could affect the data (outside of those made by the improvement team, such as a sudden loss of several staff members in a clinic).</div>
<div class="ja50-ce-glossary-entry">
<span class="ja50-ce-glossary-heading">Statistical process control (SPC)</span>
:
Statistical analysis and display (eg control charts), which helps distinguish normal, everyday, inevitable variation (“common cause variation”) from nonrandom (“special cause variation”) variation. The latter indicates something special is happening which could be caused by an improvement project or something that warrants a fuller understanding and investigation.<a title="" href="#bib17" name="back-bib17"><sup>17</sup></a></div>
</body>
</html>