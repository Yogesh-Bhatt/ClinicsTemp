<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//ES//DTD journal article DTD version 5.0.2//EN//XML" "art502.dtd" [<!ENTITY gr1 SYSTEM "gr1" NDATA IMAGE><!ENTITY gr2 SYSTEM "gr2" NDATA IMAGE>]><article docsubtype="rev"><item-info><jid>PCL</jid><aid>643</aid><ce:pii>S0031-3955(09)00066-2</ce:pii><ce:doi>10.1016/j.pcl.2009.05.012</ce:doi><ce:copyright type="full-transfer" year="2009">Elsevier Inc.</ce:copyright></item-info><ce:floats><ce:figure id="fig1"><ce:label>Fig. 1</ce:label><ce:caption><ce:simple-para>Flow chart demonstrating the stages of development, testing, and implementation of a measurement and feedback plan during a typical QI project.</ce:simple-para></ce:caption><ce:link locator="gr1"/></ce:figure><ce:figure id="fig2"><ce:label>Fig.&nbsp;2</ce:label><ce:caption><ce:simple-para>Statistical process control chart showing an improved process that can be monitored for sustainability. The chart has an annotation marking the beginning of the QI intervention.</ce:simple-para></ce:caption><ce:link locator="gr2"/></ce:figure><ce:table id="tbl1" frame="topbot" colsep="0" rowsep="0"><ce:label>Table 1</ce:label><ce:caption><ce:simple-para>Key attributes of measures to support QI projects</ce:simple-para></ce:caption><tgroup cols="2"><colspec colnum="1" colname="col1"/><colspec colnum="2" colname="col2"/><thead><row rowsep="1" valign="top"><entry>Measure Attributes</entry><entry>Considerations</entry></row></thead><tbody><row valign="top"><entry namest="col1" nameend="col2"><ce:bold>Tailored to the target audience</ce:bold></entry></row><row valign="top"><entry>Meaningful, important and relevant to target audience</entry><entry><ce:list><ce:list-item><ce:para>Address areas with substantial effect on the health of population</ce:para></ce:list-item><ce:list-item><ce:para>High burden of illness, high volume, problem-prone process, poor quality and/or high variation</ce:para></ce:list-item><ce:list-item><ce:para>Relevance to financial and strategic issues</ce:para></ce:list-item></ce:list></entry></row><row valign="top"><entry>Understandable to target audience</entry><entry>Avoid unnecessary complexity</entry></row><row valign="top"><entry>Credible for target audience</entry><entry><ce:list><ce:list-item><ce:para>Use nationally recognized practice guidelines when possible</ce:para></ce:list-item><ce:list-item><ce:para>If using nationally recognized practice guidelines, measures should account for patient preferences and clinician judgment</ce:para></ce:list-item></ce:list></entry></row><row valign="top"><entry namest="col1" nameend="col2"><ce:bold>Comprehensive (ie, includes outcome, process, and balancing measures)</ce:bold></entry></row><row valign="top"><entry>Outcome measures</entry><entry>Outcome measures address how the health care services provided to patients affect their health, functional status, and/or satisfaction</entry></row><row valign="top"><entry>Process measures</entry><entry><ce:list><ce:list-item><ce:para>Process measures address the health care services provided to patients</ce:para></ce:list-item><ce:list-item><ce:para>Consider all or none measures</ce:para></ce:list-item></ce:list></entry></row><row valign="top"><entry>Balancing measures</entry><entry>Balancing measures address potential unintended consequences of changes to processes</entry></row><row valign="top"><entry namest="col1" nameend="col2"><ce:bold>Carefully defined</ce:bold></entry></row><row valign="top"><entry>Definition assures baseline levels are neither too high nor too low</entry><entry><ce:list><ce:list-item><ce:para>If too high, difficult to show improvement</ce:para></ce:list-item><ce:list-item><ce:para>If too low, may be discouraging or cause disbelief</ce:para></ce:list-item></ce:list></entry></row><row valign="top"><entry>Definition assures measures are responsive to changes in the system</entry><entry>Minimize delay between improvements and measuring the effects of improvements</entry></row><row valign="top"><entry>Definition minimizes the impact of variation</entry><entry><ce:list><ce:list-item><ce:para>Reduce known causes of variation through stratification (eg, time of day, location/unit, staff or clinicians) to allow for more sensitivity in detecting improvement</ce:para></ce:list-item><ce:list-item><ce:para>Seasonal variation - avoid measures with this when possible or handle with rolling averages or year over year comparisons</ce:para></ce:list-item></ce:list></entry></row><row valign="top"><entry namest="col1" nameend="col2"><ce:bold>Measurement burden minimized</ce:bold></entry></row><row valign="top"><entry>A small, balanced set of measures</entry><entry><ce:list><ce:list-item><ce:para>Strive for a set of measures that describes a system as much as possible with as few measures as possible</ce:para></ce:list-item><ce:list-item><ce:para>Use or adapt existing measures when possible</ce:para></ce:list-item></ce:list></entry></row><row valign="top"><entry>Data collection built into the flow of work</entry><entry><ce:list><ce:list-item><ce:para>Understand work flow (eg, through process mapping) and find best place to collect data in work flow and best person(s) to collect</ce:para></ce:list-item><ce:list-item><ce:para>Use existing data where possible</ce:para></ce:list-item></ce:list></entry></row><row valign="top"><entry>Small sample sizes</entry><entry>20&ndash;40 observations when collected frequently are often adequate (eg, medical record abstraction on 25 patients collected monthly)</entry></row><row valign="top"><entry>Simple data collection instruments and methods</entry><entry><ce:list><ce:list-item><ce:para>Use simple and quick instruments like check sheets, checklists</ce:para></ce:list-item><ce:list-item><ce:para>Leverage technology (eg, email surveys, scannable forms)</ce:para></ce:list-item></ce:list></entry></row></tbody></tgroup><ce:legend><ce:simple-para><ce:italic>Data from</ce:italic> Refs. <ce:cross-refs refid="bib2 bib16 bib17 bib22 bib23"><ce:sup>2,16,17,22,23</ce:sup></ce:cross-refs>.</ce:simple-para></ce:legend></ce:table><ce:table id="tbl2" frame="topbot" colsep="0" rowsep="0"><ce:label>Table 2</ce:label><ce:caption><ce:simple-para>Example of project measurement set used by the UNC Division of Pediatric Gastroenterology for improving care for children with inflammatory bowel diseases</ce:simple-para></ce:caption><tgroup cols="2"><colspec colnum="1" colname="col1"/><colspec colnum="2" colname="col2"/><thead><row rowsep="1" valign="top"><entry>Measure</entry><entry>Project Goal</entry></row></thead><tbody><row valign="top"><entry namest="col1" nameend="col2">Process measures</entry></row><row valign="top"><entry><ce:hsp sp="1"/>% patients seen this month with disease activity recorded</entry><entry align="char" char=".">95%</entry></row><row valign="top"><entry><ce:hsp sp="1"/>% patients seen this month with active disease that have a documented plan to escalate therapy</entry><entry align="char" char=".">95%</entry></row><row valign="top"><entry><ce:hsp sp="1"/>% patients seen this month with classified steroid status</entry><entry align="char" char=".">95%</entry></row><row valign="top"><entry><ce:hsp sp="1"/>% chronic steroid users (per clinician) seen this month with documented plan for tapering (&plusmn; maintenance medication)</entry><entry align="char" char=".">95%</entry></row><row valign="top"><entry><ce:hsp sp="1"/>% patients seen this month with growth and nutritional status classified at visit</entry><entry align="char" char=".">95%</entry></row><row valign="top"><entry><ce:hsp sp="1"/>% patients seen this month classified as &ldquo;at risk&rdquo; or &ldquo;failure&rdquo; for nutrition with documented intervention</entry><entry align="char" char=".">95%</entry></row><row valign="top"><entry namest="col1" nameend="col2">Outcome measures</entry></row><row valign="top"><entry><ce:hsp sp="1"/>% of patients with active disease (mild, moderate, severe) measured by physician global assessment this month</entry><entry align="char" char=".">&lt; 20%</entry></row><row valign="top"><entry><ce:hsp sp="1"/>% patients classified as satisfactory growth status</entry><entry align="char" char=".">95%</entry></row><row valign="top"><entry namest="col1" nameend="col2">Balancing measure</entry></row><row valign="top"><entry><ce:hsp sp="1"/>Waiting time</entry><entry>N/A</entry></row></tbody></tgroup></ce:table><ce:table id="tbl3" frame="topbot" colsep="0" rowsep="0"><ce:label>Table 3</ce:label><ce:caption><ce:simple-para>An example of a children&apos;s hospital dashboard, which tracks the organization&apos;s performance.</ce:simple-para></ce:caption><tgroup cols="3"><colspec colnum="1" colname="col1"/><colspec colnum="2" colname="col2"/><colspec colnum="3" colname="col3"/><thead><row rowsep="1" valign="top"><entry>Strategic Category</entry><entry>Measure</entry><entry>Operational Definition</entry></row></thead><tbody><row valign="top"><entry morerows="2">People</entry><entry>New hire nursing turnover</entry><entry>% nurses leaving within 12 months of hire</entry></row><row valign="top"><entry>Faculty overall satisfaction</entry><entry>% rating &ldquo;very good&rdquo;</entry></row><row valign="top"><entry>Staff overall satisfaction</entry><entry>% rating &ldquo;very good&rdquo;</entry></row><row valign="top"><entry>Service</entry><entry>Patient/family overall satisfaction</entry><entry>% recommending UNC</entry></row><row valign="top"><entry morerows="5">Clinical Quality</entry><entry>Adverse events</entry><entry>Adverse drug events per 1000 doses</entry></row><row valign="top"><entry>Hospital associated infections</entry><entry># infections per 1000 pt days</entry></row><row valign="top"><entry>Raw mortality</entry><entry># inpatient deaths</entry></row><row valign="top"><entry>Cardiac arrests</entry><entry># cardiac arrests per 1000 pt days</entry></row><row valign="top"><entry>Chronic care management</entry><entry># patients in chronic care registry</entry></row><row valign="top"><entry>Readmission rate</entry><entry># patients readmitted within 72 hours</entry></row><row valign="top"><entry morerows="3">Finance</entry><entry>Payor mix</entry><entry>% distribution of gross revenues across insurance plans</entry></row><row valign="top"><entry>Gross charges</entry><entry>Actual cash collection on bills</entry></row><row valign="top"><entry>Fundraising</entry><entry>Amount of gifts per year</entry></row><row valign="top"><entry>RVU</entry><entry># relative value units (inpatient and outpatient)</entry></row><row valign="top"><entry morerows="1">Innovation</entry><entry>Publications</entry><entry># faculty publications per quarter</entry></row><row valign="top"><entry>Family advisory groups</entry><entry># active family advisory groups</entry></row><row valign="top"><entry morerows="3">Growth</entry><entry>Hours on diversion</entry><entry># hours PICU unable to accept new admissions</entry></row><row valign="top"><entry>Inpatient admissions</entry><entry># inpatients admitted to wards, PICU and NICU</entry></row><row valign="top"><entry>Inpatient patient days</entry><entry># inpatients in a bed</entry></row><row valign="top"><entry>Inpatient length of stay</entry><entry>Average # days patients stay in hospital</entry></row></tbody></tgroup><ce:legend><ce:simple-para><ce:italic>Courtesy of</ce:italic> North Carolina Children&apos;s Hospital, Chapel Hill, NC; with permission.</ce:simple-para></ce:legend></ce:table><ce:textbox id="tbox1"><ce:label>Box 1</ce:label><ce:caption><ce:simple-para>Key themes for effective quality improvement measurement feedback</ce:simple-para></ce:caption><ce:textbox-body><ce:sections><ce:para><ce:list><ce:list-item><ce:para>Data must be perceived by [clinicians] as valid to motivate change</ce:para></ce:list-item><ce:list-item><ce:para>It takes time to develop the credibility of data within [an organization]</ce:para></ce:list-item><ce:list-item><ce:para>The source and timeliness of data are critical to perceived validity</ce:para></ce:list-item><ce:list-item><ce:para>Benchmarking improves the meaningfulness of the data feedback</ce:para></ce:list-item><ce:list-item><ce:para>[Clinician] leaders can enhance the effectiveness of data feedback</ce:para></ce:list-item><ce:list-item><ce:para>Data feedback that profiles an individual [clinician&apos;s] practices can be effective but may be perceived as punitive</ce:para></ce:list-item><ce:list-item><ce:para>Data feedback must persist to sustain improved performance</ce:para></ce:list-item></ce:list></ce:para><ce:para><ce:italic>Data from</ce:italic> Bradley EH, Holmboe ES, Mattera JA, et al. Data feedback efforts in quality improvement: lessons learned from US hospitals. Qual Saf Health Care 2004;13(1):26&ndash;31.</ce:para></ce:sections></ce:textbox-body></ce:textbox></ce:floats><head><ce:title>Model for Improvement - Part Two: Measurement and Feedback for Quality Improvement Efforts</ce:title><ce:author-group><ce:author id="au1"><ce:given-name>Greg</ce:given-name><ce:surname>Randolph</ce:surname><ce:degrees>MD, MPH</ce:degrees><ce:cross-ref refid="aff1"><ce:sup>a</ce:sup></ce:cross-ref><ce:cross-ref refid="aff2"><ce:sup>b</ce:sup></ce:cross-ref><ce:cross-ref refid="aff3"><ce:sup>c</ce:sup></ce:cross-ref><ce:cross-ref refid="cor1"><ce:sup>&lowast;</ce:sup></ce:cross-ref><ce:e-address type="email">randolph@unc.edu</ce:e-address></ce:author><ce:author id="au2"><ce:given-name>Megan</ce:given-name><ce:surname>Esporas</ce:surname><ce:degrees>MPH</ce:degrees><ce:cross-ref refid="aff1"><ce:sup>a</ce:sup></ce:cross-ref><ce:cross-ref refid="aff2"><ce:sup>b</ce:sup></ce:cross-ref><ce:cross-ref refid="aff3"><ce:sup>c</ce:sup></ce:cross-ref></ce:author><ce:author id="au3"><ce:given-name>Lloyd</ce:given-name><ce:surname>Provost</ce:surname><ce:degrees>MS</ce:degrees><ce:cross-ref refid="aff4"><ce:sup>d</ce:sup></ce:cross-ref></ce:author><ce:author id="au4"><ce:given-name>Sara</ce:given-name><ce:surname>Massie</ce:surname><ce:degrees>MPH</ce:degrees><ce:cross-ref refid="aff1"><ce:sup>a</ce:sup></ce:cross-ref><ce:cross-ref refid="aff5"><ce:sup>e</ce:sup></ce:cross-ref></ce:author><ce:author id="au5"><ce:given-name>David G.</ce:given-name><ce:surname>Bundy</ce:surname><ce:degrees>MD, MPH</ce:degrees><ce:cross-ref refid="aff6"><ce:sup>f</ce:sup></ce:cross-ref></ce:author><ce:affiliation id="aff1"><ce:label>a</ce:label><ce:textfn>Department of Pediatrics, North Carolina Children&apos;s Center for Clinical Excellence, North Carolina Children&apos;s Hospital, CB# 7230, Chapel Hill, NC 27599-7230, USA</ce:textfn></ce:affiliation><ce:affiliation id="aff2"><ce:label>b</ce:label><ce:textfn>Department of Pediatrics, University of North Carolina at Chapel Hill, CB# 7230, Chapel Hill, NC 27599-7230, USA</ce:textfn></ce:affiliation><ce:affiliation id="aff3"><ce:label>c</ce:label><ce:textfn>Public Health Leadership Program, University of North Carolina at Chapel Hill, CB# 7230, Chapel Hill, NC 27599-7230, USA</ce:textfn></ce:affiliation><ce:affiliation id="aff4"><ce:label>d</ce:label><ce:textfn>API &ndash;Austin, 115 East Fifth Street, Suite 300, Austin, TX 78701, USA</ce:textfn></ce:affiliation><ce:affiliation id="aff5"><ce:label>e</ce:label><ce:textfn>Child Health Research Program, N.C. Translational and Clinical Sciences Institute, University of North Carolina at Chapel Hill, CB# 7230, Chapel Hill, NC 27599-7230, USA</ce:textfn></ce:affiliation><ce:affiliation id="aff6"><ce:label>f</ce:label><ce:textfn>Department of Pediatrics, Johns Hopkins University School of Medicine, CMSC 2-121, 600 N&nbsp;Wolfe Street, Baltimore, MD, USA</ce:textfn></ce:affiliation><ce:correspondence id="cor1"><ce:label>&lowast;</ce:label><ce:text>Corresponding author. Department of Pediatrics, University of North Carolina at Chapel Hill, CB# 7230, Chapel Hill, NC 27599 7230.</ce:text></ce:correspondence></ce:author-group><ce:abstract xml:lang="en"><ce:abstract-sec><ce:simple-para view="extended">Measurement and feedback are fundamental to quality improvement. There is a knowledge gap among health care professionals in knowing how to measure the impact of their quality improvement projects and how to use these data to improve care. This article presents a pragmatic approach to measurement and feedback for quality improvement efforts in local health care settings, such as hospitals or clinical practices. The authors include evidence-based strategies from health care and other industries, augmented with practical examples from the authors&apos; collective years of experience designing measurement and feedback strategies.</ce:simple-para></ce:abstract-sec></ce:abstract><ce:keywords xml:lang="en"><ce:section-title>Keywords</ce:section-title><ce:keyword><ce:text>Quality improvement</ce:text></ce:keyword><ce:keyword><ce:text>Measurement</ce:text></ce:keyword><ce:keyword><ce:text>Feedback</ce:text></ce:keyword><ce:keyword><ce:text>Health care quality</ce:text></ce:keyword><ce:keyword><ce:text>Organizational performance</ce:text></ce:keyword></ce:keywords></head><body><ce:sections><ce:section id="sec1"><ce:section-title>Why this topic and why now?</ce:section-title><ce:para>The public, government, payors, and health care professionals increasingly agree that the quality of health care in the United States is in urgent need of improvement.<ce:cross-ref refid="bib1"><ce:sup>1</ce:sup></ce:cross-ref> Measurement and feedback are fundamental aspects of quality improvement (QI); thus, national and local health care organizations are paying more attention to the selection and use of quality measures. To date, most of the attention and effort has been directed at developing measures at the national level to compare the performance of physicians and health care organizations, ie, what is referred to as national performance measures or standards. Little attention has been directed at measurement and feedback to guide QI projects, which represents the frontline of QI work. A substantial knowledge gap exists among health care professionals about how to select and use measures to guide QI projects.</ce:para><ce:para>In this article, the authors attempt to address this gap by providing evidence-based strategies from health care and other industries, augmented with practical examples from the authors&apos; collective years of experience designing measurement and feedback strategies for frontline health care improvement teams. The article&apos;s focus is on the design and development of measurement and feedback strategies for QI projects in local settings, such as health care systems, hospitals, and clinical practices. The article also briefly discusses organizational measures, such as those included in dashboards or scorecards, which guide decision-making and priority setting for system-wide improvements.</ce:para><ce:para>The authors use a broad definition of quality measures, including not only clinical, but also satisfaction and financial measures, which are critical to health care organizations.<ce:cross-ref refid="bib2"><ce:sup>2</ce:sup></ce:cross-ref> This focus aligns with the Institute of Medicine&apos;s definition of quality: &ldquo;The degree to which health services for individuals and populations increase the likelihood of desired health outcomes and are consistent with current professional knowledge.&rdquo;<ce:cross-ref refid="bib1"><ce:sup>1</ce:sup></ce:cross-ref></ce:para><ce:para>This article does not address measurement for QI research, which is typically too slow, too expensive, too much of a data collection burden, and too complex for clinicians and staff engaged in QI efforts in local settings.<ce:cross-ref refid="bib3"><ce:sup>3</ce:sup></ce:cross-ref> In addition, the authors do not address national performance standards used for quality assurance and accreditation, such as those established by the Joint Commission and the Center for Medicare &amp; Medicaid Services (CMS). These types of data are regularly collected and reported by hospitals, but they are infrequently used, and rarely sufficient, to guide improvement projects. For example, accountability measures are insufficient for QI teams because they are designed for external groups (eg, payors and regulators for comparison or public reassurance) and typically focus on only outcomes, not processes, making it impossible to link any process changes to changes in outcomes.<ce:cross-ref refid="bib3"><ce:sup>3</ce:sup></ce:cross-ref></ce:para></ce:section><ce:section id="sec2"><ce:section-title>Background and history</ce:section-title><ce:para>The current use of measurement and feedback in health care QI originates from both medicine and industrial/business QI. In the mid-1800s, two pioneers, Florence Nightingale and John Snow, introduced the use of data in health care. Nightingale used measures of mortality to document the low quality of care injured British soldiers received during the Crimean War in Turkey. These data provided a compelling argument for reform by demonstrating that better sanitation in the field hospital could prevent deaths. Subsequent field hospital reforms reduced the death rate from greater than 40% to less than 5%. Nightingale also developed graphical methods to present data, clearly demonstrating that measures provide an organized way of approaching improvements in medical and surgical practice. Nightingale&apos;s achievements led to the adoption of formal record keeping in British hospitals.</ce:para><ce:para>Also in the mid-1800s, John Snow tracked the incidence and geographic location of cholera in London. By mapping the clusters of cholera cases, Snow provided convincing evidence that homes supplied by the Broad Street water pump had a much higher incidence of cholera than homes supplied with water from other sources. Despite the lack of a clear causal explanation (Snow postulated a &ldquo;cholera poison&rdquo;), the data demonstrating the pattern of disease outbreak was convincing enough to persuade authorities to remove the handle from the Broad Street pump, which greatly reduced the incidence of cholera.</ce:para><ce:para>The first American to publish on performance measurement in health care was Ernest Codman, a surgeon at Massachusetts General Hospital. Codman advocated for hospital reform and systematic health care performance assessment, and in 1910, proposed the &ldquo;end result system of hospital standardization,&rdquo;<ce:cross-ref refid="bib4"><ce:sup>4</ce:sup></ce:cross-ref> whereby a hospital would track every patient it treated long enough to determine whether treatment was effective. By tracking patient outcomes, adverse events could be identified and changes could be made to improve the care of future patients.</ce:para><ce:para>Modern QI science originated in the 1920s and is often attributed to the work of Walter Shewhart of Bell Telephone Laboratories.<ce:cross-ref refid="bib5"><ce:sup>5</ce:sup></ce:cross-ref> Shewhart&apos;s work made measurement fundamental to QI activities. He published principles and techniques basic to the use of measurement, including concepts of statistical control, operational definitions, and visual display of data. He postulated two rules for presentation of data:<ce:list><ce:list-item><ce:label>1)</ce:label><ce:para>Data has no meaning apart from its context.</ce:para></ce:list-item><ce:list-item><ce:label>2)</ce:label><ce:para>Data contains both signal and noise; to be able to learn, one must separate the signal from the noise.</ce:para></ce:list-item></ce:list></ce:para><ce:para>One of Shewhart&apos;s colleagues, W. Edwards Deming, an American statistician and physicist, popularized his ideas in the industrial production and management sector beginning in the 1940s.<ce:cross-ref refid="bib6"><ce:sup>6</ce:sup></ce:cross-ref> Deming applied Shewhart&apos;s concepts in government and industry, described the Plan-Do-Study-Act (PDSA) Cycle as an approach to QI, and taught these concepts and methods to thousands of people, primarily in industries other than health care.</ce:para><ce:para>In the 1970s, physician Avedis Donabedian proposed a model for assessing health care quality, describing seven pillars of quality: efficacy, efficiency, optimality, acceptability, legitimacy, equity, and cost. He posited, &ldquo;<ce:italic>Structure</ce:italic> is the environment in which health care is provided, <ce:italic>process</ce:italic> is the method by which health care is provided, and <ce:italic>outcome</ce:italic> as the result of the care provided.&rdquo;<ce:cross-ref refid="bib7"><ce:sup>7</ce:sup></ce:cross-ref> Focusing on structure, process, and outcome, he emphasized the importance of measurement and evaluation of health care quality, assuring completeness and accuracy of medical records, observer bias, patient satisfaction, and cultural preferences for health care.</ce:para><ce:para>The 1980s saw an emphasis on outcome measurement in the health care literature as a result of variability in medical practice, evidence-based medicine, and regulatory agency requirements. During this time, physicians Paul Batalden and Don Berwick were among many health professionals who began to study and apply Deming&apos;s ideas to health care. In 1991, Batalden and Berwick helped form the Institute for Healthcare Improvement (IHI), which has led the application of QI science to health care in the United States and internationally.<ce:cross-ref refid="bib8"><ce:sup>8</ce:sup></ce:cross-ref> The IHI has been influential in promoting the adoption of measurement and feedback to improve the quality of health care.</ce:para></ce:section><ce:section id="sec3"><ce:section-title>The fundamentals of quality improvement projects</ce:section-title><ce:para>A cornerstone of QI science is the concept of a system.<ce:cross-refs refid="bib9 bib10 bib11 bib12"><ce:sup>9&ndash;12</ce:sup></ce:cross-refs> Systems are complex and dynamic, and a system&apos;s nature has a profound effect on the measurement and feedback of QI projects undertaken within it. A system is &ldquo;an interdependent group of items, people, or processes working together toward a common purpose&rdquo;. A system&apos;s identification of its common purpose aligns its parts. Those parts are interdependent, meaning that all parts of the system, and all relations between and among those parts, can influence system performance. Thus, QI projects require multiple measures, at multiple levels, to understand the effects of change on the different components of the system, and on the system as a whole.</ce:para><ce:para>The aim of a QI project must guide the selection of project measures. Project measures must be specific to the processes and outcomes being targeted for improvement and specific to the local target audience (within a health care system, hospital, or clinical practice). Stakeholders, those who can affect or be affected by changes in a system, especially frontline clinicians and staff, should receive feedback on the QI effort to understand how the work is helping to achieve the improvement aim. This type of measurement and feedback answers the fundamental question, &ldquo;Are the changes we are making (via QI projects) leading to improvement?&rdquo;</ce:para><ce:para>In the following sections, the authors describe the process of developing, testing, and implementing measurement and feedback strategies in a typical QI project. <ce:cross-ref refid="fig1">Fig.&nbsp;1</ce:cross-ref><ce:float-anchor refid="fig1"/> presents a summary of this process, and a glossary of QI terms used in our discussion is available at the end of the article.</ce:para></ce:section><ce:section id="sec4"><ce:section-title>Purposes of measurement and feedback</ce:section-title><ce:para>In QI, measurement and feedback are used to: (1) identify problems and establish baseline performance; (2) inform and guide QI projects; (3) select and test changes for improvement; and (4) assess progress toward organizational goals.</ce:para><ce:para>One strategy for evaluating performance and identifying potential areas for improvement is careful review of patient feedback from complaint systems, feedback forms, written and telephone surveys, and focus groups. Analysis of these data can help define gaps and create the case for conducting QI projects to address those gaps. Another mechanism for evaluating current performance is through continuous monitoring of system-level measures. For example, if performance gaps are detected in the hospital-wide infection rate, the leadership team can initiate a portfolio of improvement projects with the objective of improving this overall measure.</ce:para><ce:para>The second purpose of measurement in QI, which is the primary focus of this article, is to inform and guide QI projects. At the beginning of a QI project, a balanced set of measures, including outcome, process, and balancing measures, is established to support the team&apos;s aim statement (ie, the project goals).<ce:cross-ref refid="bib13"><ce:sup>13</ce:sup></ce:cross-ref> These measures are then reported graphically (typically monthly) on run charts (see previous article on the model for improvement for an example of a run chart). Statistical process control charts<ce:cross-ref refid="bib14"><ce:sup>14</ce:sup></ce:cross-ref> can also help monitor the progress of the project in accomplishing the improvement team&apos;s aim.</ce:para><ce:para>The third purpose of measurement and feedback is to develop, test, and implement changes. One common approach is to use PDSA cycles,<ce:cross-ref refid="bib13"><ce:sup>13</ce:sup></ce:cross-ref> where each cycle is designed to answer specific questions about changes that the improvement team is testing. The &ldquo;plan&rdquo; step of the PDSA cycle involves specifying these questions and developing measures and a data collection plan to that will answer them. The previous article about the model for improvement shows several examples of PDSA cycles. Usually the measures used in these small tests of change are specific process measures related to the change(s) being evaluated in the cycle; sometimes, they are the project outcome measures stratified for the clinicians or patients who were part of the cycle. Often, the most important part of feedback in a PDSA cycle comes from qualitative data, particularly from comments staff or patients make about the changes being tested.</ce:para><ce:para>The fourth purpose of measurement is to assess progress toward organizational goals. The authors briefly discuss the use of organizational scorecards and dashboard in a later section of this article.</ce:para></ce:section><ce:section id="sec5"><ce:section-title>Optimal attributes of quality improvement measures</ce:section-title><ce:para>Selecting measures for QI projects can be challenging. Fortunately, there is increasing knowledge and experience to help improvement teams select useful measures. In this section, the authors describe the most important attributes of QI measures based on the literature and collective experience: tailored to the target audience, comprehensive, carefully defined, and involving minimal measurement burden. These attributes are summarized in <ce:cross-ref refid="tbl1">Table 1</ce:cross-ref><ce:float-anchor refid="tbl1"/>.</ce:para><ce:section id="sec5.1"><ce:section-title>Tailoring to the Target Audience</ce:section-title><ce:para>When selecting measures, it is critical to consider the target audience (ie, those who will be viewing, using, and interpreting the data).<ce:cross-refs refid="bib15 bib16 bib17"><ce:sup>15&ndash;17</ce:sup></ce:cross-refs> The target audience will invariably include clinicians and clinical staff, so it is important that measures address high-impact clinical targets for the population of patients affected by the project (eg, using a common chronic condition like asthma for a chronic disease management improvement project).<ce:cross-refs refid="bib15 bib16"><ce:sup>15,16</ce:sup></ce:cross-refs> The target audience should always include system leaders; therefore, measures should link to high-level organizational priorities such as strategic and financial goals.<ce:cross-refs refid="bib16 bib17"><ce:sup>16,17</ce:sup></ce:cross-refs> Although there are some advantages to borrowing measures used by peer organizations, teams should be aware that some measures, especially those originating from research, can be overly complex or difficult to understand for the local target audience. An example is the Continuity of Care index (<mml:math altimg="si1.gif"><mml:mrow><mml:mi mathvariant="normal">coc&thinsp;&equals;&thinsp;</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>&Sigma;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi mathvariant="normal">&equals;1</mml:mi></mml:mrow><mml:mi mathvariant="normal">8</mml:mi></mml:msubsup><mml:msubsup><mml:mi>n</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&minus;</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>) used in many pediatric continuity of care research studies.<ce:cross-ref refid="bib18"><ce:sup>18</ce:sup></ce:cross-ref> Using a simpler measure, such as the percentage of patients seen by their primary care physician, would be much more intuitive for most clinicians.</ce:para><ce:para>Quality improvement measures must also be credible to the target audience. Measures from national organizations should be used when applicable, as long as they align well with the interests of the local target audience. Good resources for publicly available, evidence-based QI measures across a wide variety of clinical topics include the National Quality Measures Clearinghouse sponsored by the Agency for Healthcare Research and Quality,<ce:cross-ref refid="bib16"><ce:sup>16</ce:sup></ce:cross-ref> the National Quality Forum,<ce:cross-ref refid="bib19"><ce:sup>19</ce:sup></ce:cross-ref> the Institute for Healthcare Improvement,<ce:cross-ref refid="bib20"><ce:sup>20</ce:sup></ce:cross-ref> and the National Initiative for Children&apos;s Healthcare Quality.<ce:cross-ref refid="bib21"><ce:sup>21</ce:sup></ce:cross-ref> However, it is worth noting that the availability of pediatric-specific national measures lags that of adult-specific measures.</ce:para></ce:section><ce:section id="sec5.2"><ce:section-title>Including Comprehensive Measures</ce:section-title><ce:para>Three types of measures are essential to QI: (1) outcome measures, or those that address how the health care services provided to patients affect their health, functional status, and/or satisfaction; (2) process measures, which address the health care services provided to patients; and (3) balancing measures, which evaluate unintended consequences or the stability of the system being changed in the project. A balanced set of measures for a QI effort should include at least one outcome, process, and balancing measure.<ce:cross-refs refid="bib16 bib17 bib22"><ce:sup>16,17,22</ce:sup></ce:cross-refs></ce:para><ce:para>Outcome measures are significant for clinicians, as well as for leaders, who want to know the ultimate impact of a project. However, these measures can be slow to change over time, so inclusion of one or more process measures allows the team to&nbsp;see the effects of a QI effort more quickly. In addition, at the end of the project, process measures demonstrate that the intended changes were indeed implemented.</ce:para><ce:para>Process measures also illustrate the link between the changes made to the system and changes in the outcome. A common approach to calculating improvement is by an item-by-item measurement, such as the proportion of patients who receive each individual component of a health care service. Another approach, all-or-none measurement, is potentially advantageous in some situations. In this approach, the numerator is the number of patients who receive all of the measured components and the denominator is the total number of patients. All-or-none measures are ideal for a process that includes a series of critical steps, all of which must be completed to produce desirable outcomes, such as the process of inserting a central line.<ce:cross-ref refid="bib23"><ce:sup>23</ce:sup></ce:cross-ref> If all-or-none measurement is not appropriate, other types of composite measurement can be used.<ce:cross-ref refid="bib24"><ce:sup>24</ce:sup></ce:cross-ref></ce:para><ce:para>Any improvement project, which by definition involves making changes to one or more processes, can have unintended consequences. Balancing measures assess these potential unintended consequences and assure teams that they have indeed improved their overall system, rather than optimizing one part of the system at the expense of another.<ce:cross-ref refid="bib17"><ce:sup>17</ce:sup></ce:cross-ref> Balancing measures can also be important in helping address the concerns of those who are resistant to the proposed changes. For example, in a project to improve immunization delivery in a private pediatric practice, a balancing measure might be the effect of changes to this high-volume process on office visit wait times.</ce:para></ce:section><ce:section id="sec5.3"><ce:section-title>Carefully Defining Measures</ce:section-title><ce:para>It is important to define measures in such a way that the baseline levels (ie, the level of performance before the QI project starts) are neither too high (difficult to detect improvement) nor too low (may be deflating or seem implausible to the target audience). Measures must also be responsive (show improvement in a timely fashion) to the planned changes in the system, and minimize the impact of variation unrelated to process changes, thereby improving the &ldquo;signal-to-noise ratio&rdquo; for detecting improvements to the system.<ce:cross-refs refid="bib23 bib25"><ce:sup>23,25</ce:sup></ce:cross-refs> For example, a difference in wait times on Mondays versus Wednesdays related to differences in patient volumes on those days would be &ldquo;noise&rdquo; in an initiative attempting to lower overall wait times.</ce:para><ce:para>During a QI project, a measure&apos;s responsiveness to detecting change is affected by the time between piloting a change on a small scale and having the change fully implemented, as well as the time to have the change affect a patient&apos;s care and the time to sampling the affected patients. The last issue is particularly important in pediatrics because researchers often sample by age group (eg, sampling 24-month to 30-month olds for immunization status for vaccines given between 0 to 15 months). Thus, months may elapse between the introduction of changes and the measurement of their effects. Optimizing how a measure is defined and sampled can often minimize these delays. For example, changing a measure&apos;s denominator to &ldquo;all children seen in the past month with asthma&rdquo; rather than &ldquo;all children seen in the past quarter with asthma&rdquo; would allow for more rapid detection of the results of a change to asthma care processes.</ce:para></ce:section><ce:section id="sec5.4"><ce:section-title>Minimizing Measurement Burden</ce:section-title><ce:para>Finally, and most importantly, teams should minimize the burden of measurement for their QI project.<ce:cross-refs refid="bib17 bib25 bib26"><ce:sup>17,25,26</ce:sup></ce:cross-refs> Effort spent creating, collecting, displaying, analyzing, and interpreting data, although absolutely critical to improvement, is nonetheless time taken away from making changes to the system. The measurement burden is much like the burden of carrying water on a hike up a mountain: hikers must carry water on the way to the top, but carrying too much water will slow them down unnecessarily, or worse, tire them to the point of not being able to reach the top at all. A small, balanced set of five to eight measures will usually suffice for most improvement projects. <ce:cross-ref refid="tbl1">Table 1</ce:cross-ref> lists several strategies to minimize the burden of measurement, such as using data that already exist in your organization.</ce:para></ce:section><ce:section id="sec5.5"><ce:section-title>Putting it All Together: A Balancing Act</ce:section-title><ce:para>Measurement selection involves making tradeoffs among these desirable measure attributes. For example, a measure that is more responsive to changes in a system, such as measuring immunizations by 18 months rather than by 24 months, may have a very low baseline level in a particular practice. Improvement teams are well served if they are aware of these attributes and can weigh these tradeoffs as they strive to create useful, though never perfect, measures for their project. <ce:cross-ref refid="tbl2">Table 2</ce:cross-ref><ce:float-anchor refid="tbl2"/> presents an example of one project team&apos;s measure set to illustrate a product of this balancing act.</ce:para></ce:section></ce:section><ce:section id="sec6"><ce:section-title>The measurement development process</ce:section-title><ce:para>After identifying robust candidate measures with the attributes described above, it is time to commence the measurement development process. The process of measurement development involves establishing operational definitions, collecting data, pilot testing, establishing baselines, and setting goals. During this process, it is also important to plan for sustainability after the project is complete. The next section describes the measurement development process in detail.</ce:para><ce:section id="sec6.1"><ce:section-title>Establishing Operational Definitions</ce:section-title><ce:para>The first step of the measurement development is establishing operational definitions. Well-intentioned measures often fail to yield actionable information because of a lack of clear operational definitions.<ce:cross-ref refid="bib6"><ce:sup>6</ce:sup></ce:cross-ref> For example, a QI team aiming to improve the emergency department (ED) care of children with pneumonia might select the measure: &ldquo;Percentage of children with pneumonia who receive their first dose of antibiotics within four hours of arrival to the ED.&rdquo; Operational definitions for this measure would need to include, at a minimum, how target children are defined (eg, age restrictions, chronic disease exclusions); how the presence of pneumonia is defined (eg, clinical versus radiologic versus administrative [eg, billing] diagnosis); and how children are accounted for if they received antibiotics before their arrival in the ED or left the ED in less than four hours.</ce:para><ce:para>When defining measures, many QI teams have found it helpful to create a table that summarizes the relevant features of each measure. Measure name, numerator, denominator, source, frequency of collection, and inclusion and exclusion criteria are a minimal set of features, but more detailed specification is often necessary. In addition to defining individual measures, it is important to consider whether to track measures independently (eg, whether each of five steps in a ventilator care bundle were performed daily) or in an all-or-none fashion (eg, whether all five steps in the ventilator care bundle were performed daily), as stated earlier.<ce:cross-ref refid="bib10"><ce:sup>10</ce:sup></ce:cross-ref></ce:para></ce:section><ce:section id="sec6.2"><ce:section-title>Collecting Data</ce:section-title><ce:para>A critical step in developing a data collection strategy is careful consideration of the utility and accessibility of existing data sources. In many health care settings, the electronic medical record (EMR) serves as a valuable source of data. For example, an immunization improvement effort might involve querying the EMR monthly to assess the immunization status for all children of a certain age. These electronic systems are often a robust source of data, but they can be difficult to access. Using an EMR can be a challenge unless improvement team members have both administrative access to the data and the technological expertise required to extract and analyze the data.</ce:para><ce:para>Another strategy involves analyzing data collected in an existing patient registry. For example, patient registries of children with chronic health conditions can be used to monitor performance on outcome, process, and even balancing measures. One advantage of registry data is that they typically provide information on the entire population of interest (eg, all children with sickle cell disease seen in a hematology clinic). A disadvantage of registry data is that data specifications are typically already in place before the initiation of an improvement project, limiting the scope of what can be tracked to what already exists in the registry.</ce:para><ce:para>Regardless of the means of data collection, it is essential to develop a collection strategy that yields a maximal quality of actionable data with a minimal amount of extra effort. Data collected for other purposes (eg, clinic billing) can often be used in improvement efforts, and may involve substantially less time and effort to collect than data collected specifically for a QI project. Nonetheless, it is often worth comparing a small sample of billing data with more robust data sources (eg, EMR) to ensure that billing data accurately reflect the results they are intended to measure. For example, a review of 20 visits for which the billing data indicate a principal diagnosis code of bronchiolitis could be compared with EMR notes from each of the 20 visits to confirm the accuracy of the billing data as a means of identifying bronchiolitis visits.</ce:para></ce:section><ce:section id="sec6.3"><ce:section-title>Pilot Testing</ce:section-title><ce:para>Measures that appear conceptually strong often fail in the implementation process. For example, using a validated questionnaire is a logical approach to measuring and tracking patient satisfaction; however, if patient flow in the setting of interest is such that patients rarely have time to complete the survey, data collection will be problematic. By pilot testing measures and the data collection process on a small scale, improvement teams can determine whether the measurement system functions as planned. Shortening the survey, administering it at a different point in the visit, conducting it as an interview instead of a survey, or mailing it to patients might all be reasonable alternatives. These strategies would also need to be pilot tested to directly observe potential barriers to their effectiveness. Often, correctable errors (eg, &ldquo;which provider did you see today?&rdquo; inadvertently omitted from the patient satisfaction survey) can be identified after pilot testing the survey with a few patients.</ce:para></ce:section><ce:section id="sec6.4"><ce:section-title>Establishing Baselines and Setting Goals</ce:section-title><ce:para>Establishing baseline performance and setting performance goals are essential steps in developing the measurement strategy. In simple terms, measuring baseline performance answers the question, &ldquo;where are we now?&rdquo; and setting a goal establishes &ldquo;where do we want to go and how soon do we want to get there?&rdquo; The period of baseline measurement should be long enough to provide convincing data to recipients but short enough to avoid impeding initiation of QI efforts.</ce:para><ce:para>In the authors&apos; experience, QI efforts are frequently derailed by attempts to collect &ldquo;perfect&rdquo; baseline data when more concise (or approximate) data would suffice. Depending on the frequency of the events being measured, prospective versus retrospective baseline data collection may be considered. For example, in a project aimed at reducing catheter-associated bloodstream infections in a pediatric intensive care unit (ICU), historical (ie, retrospective) data from the preceding year may be the best approach for establishing an accurate baseline, given the rarity of the event. In contrast, a 1-week, prospective hand-washing observation period may provide sufficient baseline data for a hand-hygiene QI project.</ce:para><ce:para>Goal setting is typically focused on process and outcome measures and can be conceptualized in several ways. First, QI teams can set absolute goals (eg, &ldquo;90% receiving a preventive service&rdquo;), which may be consistent with external standards (such as Healthy People 2010). Absolute goals may help team members see linkages between their local efforts and larger regional and national efforts to improve care, as well as compare their performance directly against a standardized benchmark. Some teams choose to set across-the-board, though arbitrary, absolute goals for their process measures (eg, &ldquo;95% completion of all processes&rdquo;) while linking absolute goals for outcome measures to national benchmarks. Second, teams can set goals relative to baseline performance (eg, &ldquo;reduce the time required to process refill prescription requests by 50%&rdquo;). This can be particularly effective when baseline performance is poor. When the outcomes being measured are rare (eg, &ldquo;pediatric codes outside the ICU&rdquo;), goals can be conceptualized in terms of time between events (eg, &ldquo;reach 6 months with no pediatric codes outside the ICU&rdquo;). Finally, goals can be a combination of the above strategies, as well as staged over time (eg, &ldquo;reduce wait time for well child appointments to half of present wait time within 6 months, and to less than 2 days within 12 months&rdquo;). In the authors&apos; experience, whether they are staged or not, it is usually preferable to use absolute goals rather than relative goals because they are more tangible and meaningful to teams and stakeholders.</ce:para></ce:section><ce:section id="sec6.5"><ce:section-title>Planning for Sustainability</ce:section-title><ce:para>Sustainability is a key component of measurement development, and should be considered from the earliest phases. Quality improvement efforts that are sustained over long periods are usually those in which sustainability is considered early in the program&apos;s design and implementation, including the measurement strategy. What may seem like a reasonable measurement strategy when viewed in the lifespan of a QI project (eg, &ldquo;review 10 medical records each week to determine the percentage of children receiving recommended hemoglobin screening&rdquo;) may seem burdensome when extended beyond the project period. If data collection is considered &ldquo;extra&rdquo; work to the involved staff or is performed by an external research team or a team that will not operate at the completion the project, long-term sustainability of the measurement process will be unlikely. Conversely, if data collection is integrated into the everyday responsibilities of frontline staff, the likelihood of sustained data collection will increase. Successful teams often reduce the frequency of measurement (eg, from weekly to monthly or monthly to quarterly) after goal performance has been reached, as well as strive to &ldquo;hard-wire&rdquo; data collection into standard practice operations. The authors provide further discussion about implementing the sustainability plan later in this article.</ce:para></ce:section></ce:section><ce:section id="sec7"><ce:section-title>Principles of quality improvement feedback</ce:section-title><ce:para>Quality improvement measures not only document progress in QI efforts but also serve as strong motivators for improvement. However, in order for these data to resonate with and motivate frontline clinicians and staff, both the message and the messenger must be thoughtfully considered. Bradley and colleagues<ce:cross-ref refid="bib27"><ce:sup>27</ce:sup></ce:cross-ref> recently identified seven themes essential to effective feedback of data in QI projects (<ce:cross-ref refid="tbox1">Box 1</ce:cross-ref><ce:float-anchor refid="tbox1"/>). Six of the seven themes relate directly to the message of the data feedback, and the remaining theme relates to the messenger.</ce:para><ce:section id="sec7.1"><ce:section-title>The Message: Is it Believable?</ce:section-title><ce:para>The importance of the perceived validity of the feedback is highlighted in three themes: (1) data must be viewed as valid to motivate change; (2) it takes time to develop data credibility; and (3) the source and timeliness of data feedback are critical to perceived validity. Discharge diagnosis codes, for example, can be an excellent means of identifying cases of a given illness in administrative data. However, if clinicians are skeptical about the accuracy of discharge diagnosis coding, inferences drawn from such data may garner little traction for improvement. Likewise, year-old data leave room for clinicians to assert, &ldquo;things have changed since then,&rdquo; whereas real-time data are more difficult to dismiss.</ce:para></ce:section><ce:section id="sec7.2"><ce:section-title>The Message: Is it in the Proper Context and can it be Sustained?</ce:section-title><ce:para>As previously discussed, feedback can be framed in ways that enhance its effectiveness. Comparison of local data to national or other benchmarks can provide motivation for improvement. Similarly, clinician-level data documenting each individual&apos;s performance in relation to the group&apos;s performance can be valuable, though considerable care is needed to avoid creating a punitive atmosphere. Lastly, just as the sustainability of any measurement strategy must be considered in its development, the sustainability of the feedback system is equally important and must be formally integrated into the standard operating procedure of an organization if it is to persist.</ce:para></ce:section><ce:section id="sec7.3"><ce:section-title>The Message: Does it Effectively Convey the Underlying Information?</ce:section-title><ce:para>Valid and actionable data are most effective when they are presented to frontline users in visually compelling ways.<ce:cross-refs refid="bib28 bib29"><ce:sup>28,29</ce:sup></ce:cross-refs> A complex table of data may provide all the necessary information, but this display of data may make it difficult to determine whether improvements are being made. In contrast, an annotated run chart, which plots data over time, can more readily provide answers to questions such as, &ldquo;how far have we progressed toward our goal?&rdquo;, &ldquo;how much further do we have to go?&rdquo;, &ldquo;have we sustained our gains?&rdquo; and &ldquo;what change, if any, resulted in an improvement?&rdquo; Annotating run charts to indicate key events (eg, time the project team started or new change introduced) and applying statistical process control limits to distinguish random (noise) from nonrandom variation (signal) add useful detail to the visual display of performance data. These enhancements help distinguish true changes in processes or outcomes from the expected variation seen in stable systems over time.</ce:para></ce:section><ce:section id="sec7.4"><ce:section-title>The Messenger</ce:section-title><ce:para>Respected leaders, often described as &ldquo;champions,&rdquo; are frequently the most effective messengers for delivering performance data to frontline clinicians and staff.<ce:cross-refs refid="bib27 bib30"><ce:sup>27,30</ce:sup></ce:cross-refs> For example, when physician leaders provide feedback to other physicians, it provides validation that the data came from colleagues with shared interests in promoting, as well as shared barriers to providing, optimal clinical care. In contrast, when data originate from an external body, such as a QI department or payer, clinicians may be more likely to view the data as invalid, punitive, or simply irrelevant. The importance of identifying messengers who can convey the feedback in honest and motivational ways cannot be overstated and is consistent with the literature on the effectiveness of academic detailing (the use of clinical expert outreach visits to physician practices).<ce:cross-ref refid="bib31"><ce:sup>31</ce:sup></ce:cross-ref></ce:para></ce:section></ce:section><ce:section id="sec8"><ce:section-title>Assuring sustainability of ongoing measurement</ce:section-title><ce:para>Achieving sustainability of QI projects depends on continuing to measure key processes and outcomes. For QI teams, measurement provides a source of learning during implementation and a method of maintenance after implementation. Some of the measures developed and used in testing and implementation should be considered for ongoing use in the organization when the QI team disbands. However, the measures may be reported and analyzed less frequently during this ongoing monitoring phase. Measuring over time allows an organization to determine whether it is continuing to get the desired results and whether those results can be predicted to continue in the future. This process addresses the question, &rdquo;Is there a need to update the process or make new changes?&rdquo;</ce:para><ce:para>To prepare for this continued use of key measures, the QI team should consider strategies such as these:<ce:list><ce:list-item><ce:label>&squf;</ce:label><ce:para>Incorporate measurement with another existing work activity;</ce:para></ce:list-item><ce:list-item><ce:label>&squf;</ce:label><ce:para>Use existing data collection systems or develop easy-to-use data collection forms;</ce:para></ce:list-item><ce:list-item><ce:label>&squf;</ce:label><ce:para>Build measurement into EMRs, registries, or other data systems so that it is easy for administrative staff and clinicians to use as part of usual clinical care;</ce:para></ce:list-item><ce:list-item><ce:label>&squf;</ce:label><ce:para>Clearly define roles and responsibilities for ongoing data collection after the QI project is complete;</ce:para></ce:list-item><ce:list-item><ce:label>&squf;</ce:label><ce:para>Set aside time to review data with those who collect it before completion of the QI project. This will help everyone understand how the data is being used, and provide reinforcement for their efforts.</ce:para></ce:list-item></ce:list></ce:para><ce:para>Although run charts of the key measures are effective for learning during an improvement project, plotting key measures on a statistical process control chart is more useful for maintaining a change. These charts provide signals to detect whether the process is beginning to deteriorate (eg, staff reverts to practices used before the improvements were made). In addition, control charts allow teams to predict future expected performance. Statistical process control charts can be maintained for the key measures used in any improvement effort. <ce:cross-ref refid="fig2">Fig.&nbsp;2</ce:cross-ref><ce:float-anchor refid="fig2"/> shows an example of a control chart<ce:cross-ref refid="bib32"><ce:sup>32</ce:sup></ce:cross-ref> for a measure of harm during hospital admission. These maintenance charts will provide signals if the process ever begins to deteriorate, or, in this example, if data exceeds the upper control limit.</ce:para></ce:section><ce:section id="sec9"><ce:section-title>Measurement for supporting organizational performance</ce:section-title><ce:para>Just as measures are used to identify performance gaps and set priorities for improvement at the project level, health care organizations, including clinical practices, networks, hospitals, and health systems, also need a balanced set of system-level measures to track progress toward their strategic goals. Measures guide the direction and focus of QI efforts across the organization and should complement other system-level performance measures (eg, finance, use).<ce:cross-ref refid="bib33"><ce:sup>33</ce:sup></ce:cross-ref> Collectively, this set of measures should serve as both a gauge of current performance and as input into the future direction of the organization.<ce:cross-ref refid="bib34"><ce:sup>34</ce:sup></ce:cross-ref> Leaders and governing boards need to be actively involved in selecting and developing a dashboard of quality measures that reflects the culture and mission of their organization.<ce:cross-refs refid="bib30 bib35"><ce:sup>30,35</ce:sup></ce:cross-refs> To improve quality at the system-level, senior leadership needs to be committed and accountable to improving (or maintaining) performance on these measures, and prepared to build the will needed to drive change throughout the organization.<ce:cross-refs refid="bib30 bib36"><ce:sup>30,36</ce:sup></ce:cross-refs></ce:para><ce:para>Organizational performance measures should help leaders understand their progress toward accomplishing their mission. Leadership should identify a limited set of approximately 10 to 20 high-level measures that are focused on what the organization wants to accomplish and are balanced from the perspective of organization stakeholders. Dashboard measures should endure year to year; however, the strategic priorities of the organization will often focus on one or two measures each year.</ce:para><ce:para>This dashboard of measures should include operational definitions that can be easily understood at all levels of the organization, from the leadership to front-line clinicians and staff (see <ce:cross-ref refid="tbl3">Table 3</ce:cross-ref><ce:float-anchor refid="tbl3"/> for an example of a children&apos;s hospital dashboard). Like project measurement, organizational measurement must be timely (no more than a month&apos;s lag between data and review) and it should not require excessive data collection, which hinders sustainability. The data should be extracted and graphed to show patterns and trends so that improvements can be tracked over time.<ce:cross-refs refid="bib30 bib37"><ce:sup>30,37</ce:sup></ce:cross-refs> Following these principles ensures that leaders and the governing board of the health care system can continuously monitor and respond to data, and keep the organization moving toward meeting its goals.</ce:para><ce:para>Another question that system-level performance measures should help to answer is: &ldquo;How do we compare with others?&rdquo; Unlike the mission question stated above, which should be asked at every leadership and board meeting, this question should be addressed annually when reevaluating the organization&apos;s strategic goals.<ce:cross-ref refid="bib38"><ce:sup>38</ce:sup></ce:cross-ref> Benchmarking against peer organizations can help determine how the system is comparatively performing and it can identify opportunities for learning from other best practices.<ce:cross-ref refid="bib37"><ce:sup>37</ce:sup></ce:cross-ref></ce:para><ce:para>Tracking organizational performance on quality measures is increasingly relevant to the financial security of practices, networks, hospitals, and health systems. These measures are being incorporated into federal data reporting standards, such as The Joint Commission and the CMS Core Measures. CMS will link hospital reimbursement to organizations&apos; performance on the core measures, a strategy known as &ldquo;pay for performance,&rdquo; an approach that numerous payors have adopted or will soon adopt.<ce:cross-refs refid="bib39 bib40"><ce:sup>39,40</ce:sup></ce:cross-refs></ce:para></ce:section><ce:section id="sec10"><ce:section-title>Summary</ce:section-title><ce:para>Measurement and feedback are fundamental aspects of QI. The authors have described a pragmatic approach to measurement and feedback for QI efforts in local health care settings, including hospitals and clinical practices. The authors included evidence-based strategies from health care and other industries, augmented by their collective practical experience designing measurement and feedback strategies. The authors also described an approach to developing, testing, and implementing measurement and feedback strategies during the stages of a typical QI project.</ce:para><ce:para>The process as described here will assist health care professionals in knowing how to measure the effects of their QI projects and how to use these data to improve care. Health care professionals will need to understand and know how to apply the principles that are summarized in this article in order for hospitals and clinical practices to meet the growing demand to dramatically improve their performance.</ce:para></ce:section></ce:sections><ce:acknowledgment><ce:section-title>Acknowledgments</ce:section-title><ce:para>The authors are grateful for the thoughtful review and feedback provided by John B. Anderson, MD, MPH; Virginia (Ginna) Crowe, RN, EdD; Michael Steiner, MD; Jayne M. Stuart, MPH; and Jane Taylor, EdD.</ce:para></ce:acknowledgment></body><tail><ce:bibliography><ce:section-title>References</ce:section-title><ce:bibliography-sec><ce:bib-reference id="bib1"><ce:label>1</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:surname>Institute of Medicine</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Crossing the quality chasm: a new health system for the 21st century</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>2001</sb:date><sb:publisher><sb:name>National Academy Press</sb:name><sb:location>Washington, DC</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib2"><ce:label>2</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>E.C.</ce:given-name><ce:surname>Nelson</ce:surname></sb:author><sb:author><ce:given-name>J.J.</ce:given-name><ce:surname>Mohr</ce:surname></sb:author><sb:author><ce:given-name>P.B.</ce:given-name><ce:surname>Batalden</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Improving health care, part 1: the clinical value compass</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Jt Comm J Qual Improv</sb:maintitle></sb:title><sb:volume-nr>22</sb:volume-nr></sb:series><sb:date>1996</sb:date></sb:issue><sb:pages><sb:first-page>243</sb:first-page><sb:last-page>258</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib3"><ce:label>3</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>L.L.</ce:given-name><ce:surname>Solberg</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Mosser</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>McDonald</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The three faces of performance measursmement: improvement, accountability and research</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Jt Comm J Qual Improv</sb:maintitle></sb:title><sb:volume-nr>23</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>1997</sb:date></sb:issue><sb:pages><sb:first-page>135</sb:first-page><sb:last-page>147</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib4"><ce:label>4</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>E.A.</ce:given-name><ce:surname>Codman</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A study in hospital efficiency (1917)</sb:maintitle></sb:title></sb:contribution><sb:comment>Reprinted by the Joint Commission on Accreditation of Healthcare Organizations</sb:comment><sb:host><sb:book><sb:date>1996</sb:date><sb:publisher><sb:name>Oakbrook Terrace</sb:name><sb:location>Illinois</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib5"><ce:label>5</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>W.A.</ce:given-name><ce:surname>Shewhart</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The economic control of quality of manufactured product (1931)</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1980</sb:date><sb:publisher><sb:name>Reprinted by ASQC</sb:name><sb:location>Milwaukee (WI)</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib6"><ce:label>6</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>W.E.</ce:given-name><ce:surname>Deming</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Out of the crisis</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1986</sb:date><sb:publisher><sb:name>Massachusetts Institute of Technology</sb:name><sb:location>Cambridge (MA)</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib7"><ce:label>7</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Donabedian</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Bashshur</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>An introduction to quality assurance in health care</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>2003</sb:date><sb:publisher><sb:name>Oxford University Press</sb:name><sb:location>New York</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib8"><ce:label>8</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Kenney</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The best practice: how the new quality movement is transforming medicine</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>2008</sb:date><sb:publisher><sb:name>Public Affairs</sb:name><sb:location>New York</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib9"><ce:label>9</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Forrester</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Principles of systems</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1986</sb:date><sb:publisher><sb:name>Productivity Press</sb:name><sb:location>Cambridge (MA)</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib10"><ce:label>10</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>T.W.</ce:given-name><ce:surname>Nolan</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Understanding medical systems</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Annals of Internal Medicine</sb:maintitle></sb:title><sb:volume-nr>128</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>1998</sb:date></sb:issue><sb:pages><sb:first-page>293</sb:first-page><sb:last-page>298</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib11"><ce:label>11</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>W.E.</ce:given-name><ce:surname>Deming</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The new economics for industry, government, and education</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:edition>2nd edition</sb:edition><sb:date>1994</sb:date><sb:publisher><sb:name>Massachusetts Institute of Technology</sb:name><sb:location>Cambridge (MA)</sb:location></sb:publisher></sb:book></sb:host><sb:comment>p. 92&ndash;115</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib12"><ce:label>12</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Senge</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The fifth discipline: the art &amp; practice of the learning organization</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1994</sb:date><sb:publisher><sb:name>Doubleday</sb:name><sb:location>New York</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib13"><ce:label>13</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Langley</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Nolan</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Nolan</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>The improvement guide: a practical approach to enhancing organizational performance</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1996</sb:date><sb:publisher><sb:name>Jossey-Bass Pub</sb:name><sb:location>San Francisco (CA)</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib14"><ce:label>14</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:surname>Shewhart WA</ce:surname></sb:author></sb:authors></sb:contribution><sb:host><sb:edited-book><sb:editors><sb:editor><ce:given-name>W.E.</ce:given-name><ce:surname>Deming</ce:surname></sb:editor></sb:editors><sb:title><sb:maintitle>Statistical method from the viewpoint of quality control (1939)</sb:maintitle></sb:title><sb:date>1986</sb:date><sb:publisher><sb:name>Dover Press</sb:name><sb:location>New York</sb:location></sb:publisher></sb:edited-book><sb:pages><sb:first-page>1</sb:first-page><sb:last-page>15</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib15"><ce:label>15</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:surname>Joint Commission on Accreditation of Health Care Organizations</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Attributes of core performance measures and associated evaluation criteria</sb:maintitle></sb:title></sb:contribution><sb:comment>Available at:</sb:comment><sb:host><sb:e-host><ce:inter-ref xlink:href="http://www.jointcommission.org/PerformanceMeasurement/PerformanceMeasurement/">http://www.jointcommission.org/PerformanceMeasurement/PerformanceMeasurement/</ce:inter-ref><sb:date>2008</sb:date></sb:e-host></sb:host><sb:comment>Accessed November 18, 2008</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib16"><ce:label>16</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:surname>NQMC Agency for Healthcare Research and Quality</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>National quality measures clearinghouse</sb:maintitle></sb:title></sb:contribution><sb:comment>Available at:</sb:comment><sb:host><sb:e-host><ce:inter-ref xlink:href="http://www.qualitymeasures.ahrq.gov/">http://www.qualitymeasures.ahrq.gov/</ce:inter-ref><sb:date>2008</sb:date></sb:e-host></sb:host><sb:comment>Accessed November 18, 2008</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib17"><ce:label>17</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:surname>Association of Public Health Observatories</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The good indicators guide: understanding how to use and choose indicators. NHS Institute for Innovation and Improvement</sb:maintitle></sb:title></sb:contribution><sb:comment>Available at:</sb:comment><sb:host><sb:e-host><ce:inter-ref xlink:href="http://www.apho.org.uk/resource/item.aspx?RID&equals;44584">http://www.apho.org.uk/resource/item.aspx?RID&equals;44584</ce:inter-ref><sb:date>2008</sb:date></sb:e-host></sb:host><sb:comment>Accessed November 18, 2008</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib18"><ce:label>18</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>D.A.</ce:given-name><ce:surname>Christakis</ce:surname></sb:author><sb:author><ce:given-name>J.A.</ce:given-name><ce:surname>Wright</ce:surname></sb:author><sb:author><ce:given-name>F.J.</ce:given-name><ce:surname>Zimmerman</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Continuity of care is associated with well-coordinated care</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Ambul Pediatr</sb:maintitle></sb:title><sb:volume-nr>3</sb:volume-nr></sb:series><sb:issue-nr>2</sb:issue-nr><sb:date>2003</sb:date></sb:issue><sb:pages><sb:first-page>82</sb:first-page><sb:last-page>86</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib19"><ce:label>19</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:surname>National Quality Forum</ce:surname></sb:author></sb:authors></sb:contribution><sb:comment>Available at:</sb:comment><sb:host><sb:e-host><ce:inter-ref xlink:href="http://www.qualityforum.org/">http://www.qualityforum.org/</ce:inter-ref><sb:date>2008</sb:date></sb:e-host></sb:host><sb:comment>Accessed November 18, 2008</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib20"><ce:label>20</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:surname>Institute for Healthcare Improvement</ce:surname></sb:author></sb:authors></sb:contribution><sb:comment>Available at:</sb:comment><sb:host><sb:e-host><ce:inter-ref xlink:href="http://www.ihi.org">http://www.ihi.org</ce:inter-ref><sb:date>2008</sb:date></sb:e-host></sb:host><sb:comment>Accessed November 18, 2008</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib21"><ce:label>21</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:surname>National Initiative for Children&apos;s Healthcare Quality</ce:surname></sb:author></sb:authors></sb:contribution><sb:comment>Available at:</sb:comment><sb:host><sb:e-host><ce:inter-ref xlink:href="http://www.nichq.org">http://www.nichq.org</ce:inter-ref><sb:date>2008</sb:date></sb:e-host></sb:host><sb:comment>Accessed November 18, 2008</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib22"><ce:label>22</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:surname>American Academy of Pediatrics Steering Committee on Quality Improvement and Management and Committee on Practice and Ambulatory Medicine</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Principles for the development and use of quality measures. Policy statement</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Pediatrics</sb:maintitle></sb:title><sb:volume-nr>121</sb:volume-nr></sb:series><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>411</sb:first-page><sb:last-page>418</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib23"><ce:label>23</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Nolan</ce:surname></sb:author><sb:author><ce:given-name>D.M.</ce:given-name><ce:surname>Berwick</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>All-or-none measurement raises the bar on performance</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>JAMA</sb:maintitle></sb:title><sb:volume-nr>295</sb:volume-nr></sb:series><sb:issue-nr>10</sb:issue-nr><sb:date>2006</sb:date></sb:issue><sb:pages><sb:first-page>1168</sb:first-page><sb:last-page>1170</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib24"><ce:label>24</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Reeves</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Campbell</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Adams</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Combining multiple indicators of clinical quality: an evaluation of different analytic approaches</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Med Care</sb:maintitle></sb:title><sb:volume-nr>45</sb:volume-nr></sb:series><sb:issue-nr>6</sb:issue-nr><sb:date>2007</sb:date></sb:issue><sb:pages><sb:first-page>489</sb:first-page><sb:last-page>496</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib25"><ce:label>25</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>E.C.</ce:given-name><ce:surname>Nelson</ce:surname></sb:author><sb:author><ce:given-name>M.E.</ce:given-name><ce:surname>Splaine</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Plume</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Good measurement for good improvement work</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Qual Manag Health Care</sb:maintitle></sb:title><sb:volume-nr>13</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2004</sb:date></sb:issue><sb:pages><sb:first-page>1</sb:first-page><sb:last-page>16</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib26"><ce:label>26</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>P.J.</ce:given-name><ce:surname>Pronovost</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Nolan</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Zeger</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>How can clinicians measure safety and quality in acute care?</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Lancet</sb:maintitle></sb:title><sb:volume-nr>363</sb:volume-nr></sb:series><sb:date>2004</sb:date></sb:issue><sb:pages><sb:first-page>1061</sb:first-page><sb:last-page>1067</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib27"><ce:label>27</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>E.H.</ce:given-name><ce:surname>Bradley</ce:surname></sb:author><sb:author><ce:given-name>E.S.</ce:given-name><ce:surname>Holmboe</ce:surname></sb:author><sb:author><ce:given-name>J.A.</ce:given-name><ce:surname>Mattera</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Data feedback efforts in quality improvement: lessons learned from US hospitals</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Qual Saf Health Care</sb:maintitle></sb:title><sb:volume-nr>13</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2004</sb:date></sb:issue><sb:pages><sb:first-page>26</sb:first-page><sb:last-page>31</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib28"><ce:label>28</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>R.G.</ce:given-name><ce:surname>Carey</ce:surname></sb:author><sb:author><ce:given-name>R.C.</ce:given-name><ce:surname>Lloyd</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Measuring quality improvement in healthcare: a guide to statistical process control application</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>2001</sb:date><sb:publisher><sb:name>Quality Press</sb:name><sb:location>Wisconsin</sb:location></sb:publisher></sb:book></sb:host><sb:comment>p. 43&ndash;150</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib29"><ce:label>29</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>E.R.</ce:given-name><ce:surname>Tufte</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The visual display of quantitative information</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:edition>2nd edition</sb:edition><sb:date>2001</sb:date><sb:publisher><sb:name>Graphics Press</sb:name><sb:location>Cheshire (CT)</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib30"><ce:label>30</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>C.R.</ce:given-name><ce:surname>Denham</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Leaders need dashboards, dashboards need leaders</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J Patient Saf</sb:maintitle></sb:title><sb:volume-nr>2</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2006</sb:date></sb:issue><sb:pages><sb:first-page>45</sb:first-page><sb:last-page>53</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib31"><ce:label>31</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>M.A.</ce:given-name><ce:surname>O&apos;Brien</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Rogers</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Jamtvedt</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Educational outreach visits: effects on professional practice and health care outcomes</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Cochrane Database Syst Rev</sb:maintitle></sb:title></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>2007</sb:date></sb:issue></sb:host><sb:comment>CD000409. DOI: 10.1002/14651858</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib32"><ce:label>32</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Lloyd</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Quality health care: a guide to developing and using indicators</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>2004</sb:date><sb:publisher><sb:name>Jones &amp; Bartlett Publishers</sb:name><sb:location>Boston</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib33"><ce:label>33</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>L.A.</ce:given-name><ce:surname>Martin</ce:surname></sb:author><sb:author><ce:given-name>E.C.</ce:given-name><ce:surname>Nelson</ce:surname></sb:author><sb:author><ce:given-name>R.C.</ce:given-name><ce:surname>Lloyd</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Whole system measures. IHI innovation series white paper</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>2007</sb:date><sb:publisher><sb:name>Institute for Healthcare Improvement</sb:name><sb:location>Cambridge (MA)</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib34"><ce:label>34</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Provost</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Leddick</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>How to take multiple measures to get a complete picture of organizational performance</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Natl Prod Rev</sb:maintitle></sb:title><sb:volume-nr>12</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>1993</sb:date></sb:issue><sb:pages><sb:first-page>477</sb:first-page><sb:last-page>490</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib35"><ce:label>35</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Kroch</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Vaughn</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Koepke</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Hospital boards and quality dashboards</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J&nbsp;Patient Saf</sb:maintitle></sb:title><sb:volume-nr>2</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2006</sb:date></sb:issue><sb:pages><sb:first-page>10</sb:first-page><sb:last-page>19</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib36"><ce:label>36</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Conway</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Getting boards on board: engaging governing boards in quality and safety</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Jt Comm J Qual Patient Saf</sb:maintitle></sb:title><sb:volume-nr>34</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>214</sb:first-page><sb:last-page>220</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib37"><ce:label>37</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>J.L.</ce:given-name><ce:surname>Reinertsen</ce:surname></sb:author><sb:author><ce:given-name>M.D.</ce:given-name><ce:surname>Pugh</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Bisognano</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Institute for Healthcare Improvement. IHI innovation series. Seven leadership leverage points for organization-level improvement in health care</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>2005</sb:date><sb:publisher><sb:name>Institute for Healthcare Improvement</sb:name><sb:location>Cambridge (MA)</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib38"><ce:label>38</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:given-name>J.L.</ce:given-name><ce:surname>Reinertsen</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>From the top: getting the board on board. IHI conference: boards, dashboards, and data. Boston</sb:maintitle></sb:title></sb:contribution><sb:comment>Available at:</sb:comment><sb:host><sb:e-host><ce:inter-ref xlink:href="http://www.ihi.org/IHI/Topics/LeadingSystemImprovement/Leadership/EmergingContent/BoardsDashboardsData.htm">http://www.ihi.org/IHI/Topics/LeadingSystemImprovement/Leadership/EmergingContent/BoardsDashboardsData.htm</ce:inter-ref><sb:date>2007</sb:date></sb:e-host></sb:host><sb:comment>Accessed November 10, 2008</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib39"><ce:label>39</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:surname>The Joint Commission</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Performance measurement initiatives</sb:maintitle></sb:title></sb:contribution><sb:comment>Available at:</sb:comment><sb:host><sb:e-host><ce:inter-ref xlink:href="http://www.jointcommission.org/PerformanceMeasurement/PerformanceMeasurement/default.htm">http://www.jointcommission.org/PerformanceMeasurement/PerformanceMeasurement/default.htm</ce:inter-ref></sb:e-host></sb:host><sb:comment>Accessed November 10, 2008</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib40"><ce:label>40</ce:label><sb:reference><sb:contribution><sb:authors><sb:author><ce:surname>Kaiser Daily Health Policy Report</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Medicare stops paying for 10 reasonably preventable medical errors</sb:maintitle></sb:title></sb:contribution><sb:comment>Available at:</sb:comment><sb:host><sb:e-host><ce:inter-ref xlink:href="http://www.kaisernetwork.org/daily_reports/rep_index.cfm?hint&equals;3&amp;;DR_ID&equals;54758">http://www.kaisernetwork.org/daily_reports/rep_index.cfm?hint&equals;3&amp;;DR_ID&equals;54758</ce:inter-ref></sb:e-host></sb:host><sb:comment>Accessed October 1, 2008</sb:comment></sb:reference></ce:bib-reference></ce:bibliography-sec></ce:bibliography><ce:glossary><ce:section-title>Glossary</ce:section-title><ce:glossary-sec><ce:glossary-entry><ce:glossary-heading><ce:bold>Balanced set of measures</ce:bold></ce:glossary-heading><ce:glossary-def>A set of measures which, taken together, reflect as much of a system as possible without duplication, overlap or gaps.</ce:glossary-def><ce:cross-ref refid="bib17"><ce:sup>17</ce:sup></ce:cross-ref></ce:glossary-entry><ce:glossary-entry><ce:glossary-heading><ce:bold>Benchmark</ce:bold></ce:glossary-heading><ce:glossary-def>An externally agreed-upon comparator to compare performance between similar organizations or systems.</ce:glossary-def><ce:cross-ref refid="bib17"><ce:sup>17</ce:sup></ce:cross-ref></ce:glossary-entry><ce:glossary-entry><ce:glossary-heading><ce:bold>Composite indicator</ce:bold></ce:glossary-heading><ce:glossary-def>An aggregation of numerous indicators that aims to give a one-figure indicator in order to summarize measures further.</ce:glossary-def></ce:glossary-entry><ce:glossary-entry><ce:glossary-heading><ce:bold>Control charts</ce:bold></ce:glossary-heading><ce:glossary-def>A graphical tool for displaying the results of statistical process control.</ce:glossary-def></ce:glossary-entry><ce:glossary-entry><ce:glossary-heading><ce:bold>Control limits</ce:bold></ce:glossary-heading><ce:glossary-def>Define the area representing random (also called &ldquo;common cause&rdquo;) variation on either side of the centerline, plotted on a control chart.</ce:glossary-def></ce:glossary-entry><ce:glossary-entry><ce:glossary-heading><ce:bold>Dashboard</ce:bold></ce:glossary-heading><ce:glossary-def>A tool used for collecting and reporting data on system-level measures that demonstrate the overall quality of a health system over time. Dashboards provide a quick summary of structural, process and outcome performance.</ce:glossary-def></ce:glossary-entry><ce:glossary-entry><ce:glossary-heading><ce:bold>Plan-Do-Study-Act (PDSA) Cycle</ce:bold></ce:glossary-heading><ce:glossary-def>A quality improvement method consisting of four repetitive steps for learning and improvement. Plan: develop a plan to learn, test, or implement a change; Do: execute the plan; Study: compare predictions to results and document learning; Act: make changes based on learning and set up next cycle. Most changes require several PDSA cycles.</ce:glossary-def></ce:glossary-entry><ce:glossary-entry><ce:glossary-heading><ce:bold>Run chart</ce:bold></ce:glossary-heading><ce:glossary-def>A time series graph where the X-axis represents time longitudinally and the measure value is on the Y-axis. Run charts often include a median for the data points and can be augmented by inserting comments (annotations) at the point in time where process changes are made by the improvement team or other changes occur that could affect the data (outside of those made by the improvement team, such as a sudden loss of several staff members in a clinic).</ce:glossary-def></ce:glossary-entry><ce:glossary-entry><ce:glossary-heading><ce:bold>Statistical process control (SPC)</ce:bold></ce:glossary-heading><ce:glossary-def>Statistical analysis and display (eg control charts), which helps distinguish normal, everyday, inevitable variation (&ldquo;common cause variation&rdquo;) from nonrandom (&ldquo;special cause variation&rdquo;) variation. The latter indicates something special is happening which could be caused by an improvement project or something that warrants a fuller understanding and investigation.</ce:glossary-def><ce:cross-ref refid="bib17"><ce:sup>17</ce:sup></ce:cross-ref></ce:glossary-entry></ce:glossary-sec></ce:glossary></tail></article>